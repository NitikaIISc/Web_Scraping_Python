{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Part 3: https://www.youtube.com/watch?v=4zE9HjPIqC4\n",
    "\n",
    "#Here we parse the filingsummary.xml for each of the reports submitted\n",
    "##https://github.com/areed1192/sigma_coding_youtube/blob/master/python/python-finance/sec-web-scraping/Web%20Scraping%20SEC%20-%2010K%20Landing%20Page%20-%20Single.ipynb\n",
    "#import libraries\n",
    "import requests #to make url request\n",
    "import pandas as pd #transform data to more user-friendly version\n",
    "from bs4 import BeautifulSoup #for parsing\n",
    "import numpy as np\n",
    "import time\n",
    "##Beautiful Soup is powerful because our Python objects match the nested structure of the HTML document we are scraping.\n",
    "##For deeply nested HTML documents, navigation could quickly become tedious. Luckily, Beautiful Soup comes with a search \n",
    "# function so we don't have to navigate to retrieve HTML elements.\n",
    "##The find_all() method takes an HTML tag as a string argument and returns the list of elements \n",
    "# that match with the provided tag. For example, if we want all a tags in doc.html:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Importing merged Tyson data to extract information about all tickers of the companies we are interested in\n",
    "#companies_data = pd.read_csv(r\"C:/UCLAAnderson/10KfilingsTables/Merged_ofg_data_TysonAugust302022.csv\", encoding='latin-1')\n",
    "companies_data = pd.read_csv(r\"C:/UCLAAnderson/10KfilingsTables/Data/AutomatedFinancialData/SP500ListCIKImportedfromWikipedia.csv\", encoding='latin-1')\n",
    "##formerly Feb242023_TruncatedAggregation.csv\n",
    "companies_data.columns.tolist() ##colnames\n",
    "## All top companies we are interested in\n",
    "# comp_tickers = companies_data['CIK'].str.cat(sep=',').split(',')## to combine multiple rows of strings into one using pandas\n",
    "\n",
    "# type(comp_tickers)\n",
    "# xs = comp_tickers\n",
    "# s = ','.join(xs)\n",
    "# s\n",
    "\n",
    "# companies_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(companies_data)\n",
    "companies_data['Security'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Getting CIK for all the TIKR symbols we need\n",
    "# TIKR2CIK_url = 'https://www.sec.gov/files/company_tickers.json'\n",
    "# content = requests.get(TIKR2CIK_url, headers = {'user-agent':'UCLA'})\n",
    "# doc_content = content.json()\n",
    "# doc_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rel_Companies = []\n",
    "Rel_CIKCompanies = []\n",
    "Rel_comp = {}\n",
    "Companies = []\n",
    "All22Comps=pd.DataFrame()\n",
    "for idx, x in enumerate(list(doc_content.values())):\n",
    "    if x['ticker'] in comp_tickers:\n",
    "#         print(x)\n",
    "        Companies.append(x)\n",
    "        Rel_comp['CIK']= x['cik_str']\n",
    "        \n",
    "        Rel_CIKCompanies.append(Rel_comp['CIK'])\n",
    "        print(Rel_CIKCompanies)#This is a list\n",
    "        Rel_comp['TIKR']= x['ticker']\n",
    "        Rel_comp['CompanyName']= x['title']\n",
    "        \n",
    "        All22Comps = pd.concat([All22Comps,pd.DataFrame(Companies)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "comp_names = companies_data['SEC.Filing.Name'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lowCompNames = [x.lower() for x in comp_names]\n",
    "lowCompNames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import re\n",
    "CompNamesnoPuncLower=[]\n",
    "\n",
    "for eachComp in list(range(0,len(lowCompNames),1)):\n",
    "# initializing string\n",
    "    test_str = lowCompNames[eachComp]\n",
    "\n",
    "    # printing original string\n",
    "    print(\"The original string is : \" + test_str)\n",
    "\n",
    "    # Removing punctuations in string\n",
    "    # Using regex\n",
    "    res = re.sub(r'[^\\w\\s]', '', test_str)\n",
    "\n",
    "    # printing result\n",
    "    print(\"The string after punctuation filter : \" + res)\n",
    "    \n",
    "    CompNamesnoPuncLower.append(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CompNamesnoPuncLower"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rel_Companies = []\n",
    "Rel_CIKCompanies = []\n",
    "Rel_comp = {}\n",
    "Companies = []\n",
    "All22Comps = pd.DataFrame()\n",
    "\n",
    "for idx, x in enumerate(list(doc_content.values())):\n",
    "#     print(x['title'].lower())\n",
    "    \n",
    "    if x['title'].lower() in CompNamesnoPuncLower:\n",
    "        print(x['title'].lower())\n",
    "        Companies.append(x)\n",
    "        Rel_comp['CIK'] = x['cik_str']\n",
    "        Rel_CIKCompanies.append(Rel_comp['CIK'])\n",
    "        print(Rel_CIKCompanies)  # This is a list\n",
    "        Rel_comp['CompanyName'] = x['title']\n",
    "\n",
    "        All22Comps = pd.concat([All22Comps, pd.DataFrame(Companies)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All22Comps\n",
    "All22Comps.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def search_cik(company_name):\n",
    "    base_url = \"https://www.edgarcompany.sec.gov/servlet/CompanyDBSearch?page=main\"\n",
    "    search_url = f\"{base_url}&companyname={company_name}\"\n",
    "\n",
    "    try:\n",
    "        response = requests.get(search_url)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        cik_table = soup.find(\"table\", class_=\"companyResultTable\")\n",
    "        rows = cik_table.find_all(\"tr\")\n",
    "\n",
    "        cik_results = []\n",
    "        for row in rows[1:]:\n",
    "            columns = row.find_all(\"td\")\n",
    "            cik = columns[0].text.strip()\n",
    "            company = columns[1].text.strip()\n",
    "            cik_results.append({\"CIK\": cik, \"Company\": company})\n",
    "\n",
    "        return cik_results\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error retrieving CIK for {company_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "company_name = \"Apple Inc.\"\n",
    "cik_results = search_cik(company_name)\n",
    "\n",
    "if cik_results:\n",
    "    print(f\"CIK results for '{company_name}':\")\n",
    "    for result in cik_results:\n",
    "        print(f\"CIK: {result['CIK']}, Company: {result['Company']}\")\n",
    "else:\n",
    "    print(f\"No CIK results found for '{company_name}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for each in doc_content.values():\n",
    "    print(each['title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "companies_data['SEC.Filing.Name'].tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import requests\n",
    "\n",
    "def get_cik(company_names):\n",
    "    base_url = \"https://www.sec.gov/cgi-bin/browse-edgar\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
    "    }\n",
    "    cik_dict = {}\n",
    "\n",
    "    for company_name in company_names:\n",
    "        params = {\n",
    "            \"action\": \"getcompany\",\n",
    "            \"CIK\": \"\",\n",
    "            \"output\": \"atom\",\n",
    "            \"company\": company_name\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(base_url, headers={'user-agent':'UCLA'}, params=params)\n",
    "            response.raise_for_status()\n",
    "            cik = None\n",
    "            \n",
    "            # Extract CIK from the response\n",
    "            for line in response.text.splitlines():\n",
    "                if \"<cik>\" in line:\n",
    "                    cik = line.split(\"<cik>\")[1].split(\"</cik>\")[0]\n",
    "                    break\n",
    "            \n",
    "            if cik is not None:\n",
    "                cik_dict[company_name] = cik\n",
    "            else:\n",
    "                cik_dict[company_name] = \"CIK not found for the company.\"\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error retrieving CIK for {company_name}: {e}\")\n",
    "            cik_dict[company_name] = None\n",
    "\n",
    "    return cik_dict\n",
    "\n",
    "# Example usage\n",
    "company_names = [\"Apple Inc.\", \"Microsoft Corporation\", \"Amazon.com Inc.\"]\n",
    "cik_dict = get_cik(company_names)\n",
    "\n",
    "# Print CIK values\n",
    "for company_name, cik in cik_dict.items():\n",
    "    print(f\"CIK for {company_name}: {cik}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "def get_cik(company_names):\n",
    "    base_url = \"https://www.sec.gov/cgi-bin/browse-edgar\"\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"}\n",
    "    cik_dict = {}\n",
    "\n",
    "    for company_name in company_names:\n",
    "        params = {\n",
    "            \"action\": \"getcompany\",\n",
    "            \"CIK\": \"\",\n",
    "            \"output\": \"atom\",\n",
    "            \"company\": company_name\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(base_url, headers=headers, params=params)\n",
    "            response.raise_for_status()\n",
    "            cik = None\n",
    "            \n",
    "            # Extract CIK from the response\n",
    "            for line in response.text.splitlines():\n",
    "                if \"<cik>\" in line:\n",
    "                    cik = line.split(\"<cik>\")[1].split(\"</cik>\")[0]\n",
    "                    break\n",
    "            \n",
    "            if cik is not None:\n",
    "                cik_dict[company_name] = cik\n",
    "            else:\n",
    "                cik_dict[company_name] = \"CIK not found for the company.\"\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error retrieving CIK for {company_name}: {e}\")\n",
    "            cik_dict[company_name] = None\n",
    "        \n",
    "        time.sleep(1)  # Delay for 1 second between requests\n",
    "        \n",
    "    return cik_dict\n",
    "\n",
    "# Example usage\n",
    "company_names = companies_data['SEC.Filing.Name'].tolist()\n",
    "cik_dict = get_cik(company_names)\n",
    "\n",
    "# Print CIK values\n",
    "for company_name, cik in cik_dict.items():\n",
    "    print(f\"CIK for {company_name}: {cik}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting CIK for all the TIKR symbols we need\n",
    "TIKR2CIK_url = 'https://www.sec.gov/files/company_tickers.json'\n",
    "content = requests.get(TIKR2CIK_url, headers = {'user-agent':'UCLA'})\n",
    "doc_content = content.json()\n",
    "doc_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "type(doc_content)\n",
    "doc_content.items()\n",
    "doc_content[\"0\"]\n",
    "doc_content.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SourceInfo=pd.DataFrame(doc_content.values(), columns=['cik_str', 'ticker','title'])\n",
    "# SourceInfo.to_csv(\"C:/UCLAAnderson/10KfilingsTables/Data/AutomatedFinancialData/FinancialsOutput/May12_AllSourceCompanyMetadata.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Subsetted the entire nested dictionary based on the tickers that were extracted from Tyson's list\n",
    "\n",
    "Rel_Companies = []\n",
    "Rel_CIKCompanies = []\n",
    "Rel_comp = {}\n",
    "Companies = []\n",
    "All500Comps=pd.DataFrame()\n",
    "for idx, x in enumerate(list(doc_content.values())):\n",
    "    if x['ticker'] in comp_tickers:\n",
    "#         print(x)\n",
    "        Companies.append(x)\n",
    "        Rel_comp['CIK']= x['cik_str']\n",
    "        \n",
    "        Rel_CIKCompanies.append(Rel_comp['CIK'])\n",
    "        print(Rel_CIKCompanies)#This is a list\n",
    "        Rel_comp['TIKR']= x['ticker']\n",
    "        Rel_comp['CompanyName']= x['title']\n",
    "        \n",
    "        All500Comps = pd.concat([All500Comps,pd.DataFrame(Companies)])\n",
    "#         print(Rel_comp)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set_A=set(comp_tickers)\n",
    "set_B = set(All500Comps['title'])\n",
    "# set_A.difference(set_B) #for (A - B)\n",
    "# len(set(All500Comps['title']))\n",
    "\n",
    "type(comp_tickers)\n",
    "\n",
    "diff=list(set(comp_tickers) - set(All500Comps['ticker']))\n",
    "\n",
    "Missing22= companies_data[companies_data['Ticker.Symbol'].isin(diff)][['SEC.Filing.Name','Ticker.Symbol']]\n",
    "\n",
    "Missing22.to_csv('C:/UCLAAnderson/10KfilingsTables/Data/AutomatedFinancialData/Missing22Tickers.csv')\n",
    "# len(companies_data[companies_data['Ticker.Symbol'].isin(diff)][['SEC.Filing.Name','Ticker.Symbol']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "type(Rel_comp)\n",
    "Rel_comp.keys()\n",
    "pd.DataFrame(Companies)\n",
    "All500Comps.drop_duplicates().to_csv(\"C:/UCLAAnderson/10KfilingsTables/Data/AutomatedFinancialData/FinancialsOutput/May16_All500CompanyMetadata.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# print(Companies)\n",
    "# len(Companies)\n",
    "type(Companies)\n",
    "pd.DataFrame(Companies).to_csv('C:/UCLAAnderson/10KfilingsTables/Data/May16_2023_CIKwithTIKRComps.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export list of dictionaries to csv\n",
    "\n",
    "import csv\n",
    "\n",
    "to_csv = Companies\n",
    "\n",
    "keys = to_csv[0].keys()\n",
    "\n",
    "with open('C:/UCLAAnderson/10KfilingsTables/OurRelevantCompanySubset.csv', 'w', newline='') as output_file:\n",
    "    dict_writer = csv.DictWriter(output_file, keys)\n",
    "    dict_writer.writeheader()\n",
    "    dict_writer.writerows(to_csv)\n",
    "    \n",
    "    \n",
    "    \n",
    "import pandas as pd\n",
    "df = pd.DataFrame(Rel_CIKCompanies)#, columns=[\"CIK\"])\n",
    "df.to_csv('C:/UCLAAnderson/10KfilingsTables/May16_2023_TikrCikOfRelCompanies.csv', index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "len(Rel_CIKCompanies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SubComps_urls = []\n",
    "master_list = []\n",
    "for relevantCIK in Rel_CIKCompanies:\n",
    "    # base URL for the SEC Edgar browser\n",
    "    endpoint = r\"https://www.sec.gov/cgi-bin/browse-edgar\"\n",
    "\n",
    "    ## defining a new variable parameter dictionary that will take the parameters' payload\n",
    "    param_dict = {'action':'getcompany',\n",
    "                  'CIK':relevantCIK,\n",
    "                  'type':'10-K',\n",
    "                  'dateb':'20220101',#####<<<<<<<<<<< before year 2023\n",
    "                  'owner':'exclude',\n",
    "                  'start':'',\n",
    "                  'output':'',#html default\n",
    "                  'count':5   \n",
    "    }\n",
    "\n",
    "\n",
    "    ## We will save the content that is sent back to us in a response variable:\n",
    "    #  and then we will define the url as the endpoint and set out parameters to the param_dict\n",
    "    response1 = requests.get(url = endpoint, headers = {'user-agent':'UCLA'}, params = param_dict)\n",
    "    print('Request1 Successful')\n",
    "    print(response1.url)\n",
    "    SubComps_urls.append(response1.url)\n",
    "    \n",
    "    \n",
    "    ##Parsing that response content and specify the parser - we know it is going to be html that comes back:\n",
    "    soup = BeautifulSoup(response1.content,'html.parser')\n",
    "    # find the document table with our data-- check using inspect how the table of all files is coded--it \n",
    "    #  has class = tablefile2 in the name\n",
    "\n",
    "    doc_table = soup.find_all('table', class_='tableFile2')\n",
    "    doc_table\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "len(doc_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DIRECTLY USING CIKs I GATHERED - 20 OUT OF 500 COMPANIES IS JUST 4% OF ALL COMPANIES - AND THIS IS INCREDIBLY SUCCESSFUL AUTOMATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rel_CIKCompanies=companies_data['CIK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rel_CIKCompanies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Getting URLs for all relevant company subset using CIKs extracted earlier:\n",
    "SubComps_Suburls = []\n",
    "master_links = []\n",
    "import time\n",
    "start_time = time.time()\n",
    "# main()\n",
    "statements_url = [] ##empty list\n",
    "statements_urlDF=[]\n",
    "\n",
    "SecondResponses=[]\n",
    "AllSoup2=[]\n",
    "\n",
    "for relevantCIK in Rel_CIKCompanies:\n",
    "    # base URL for the SEC Edgar browser\n",
    "    endpoint = r\"https://www.sec.gov/cgi-bin/browse-edgar\"\n",
    "\n",
    "    ## defining a new variable parameter dictionary that will take the parameters' payload\n",
    "    param_dict = {'action':'getcompany',\n",
    "                  'CIK':relevantCIK,\n",
    "                  'type':'10-K',\n",
    "                  'dateb':'20230101',#YYYMMDD<<<<<<<<<<< before year 2023\n",
    "                  'owner':'exclude',\n",
    "                  'start':'',\n",
    "                  'output':'',#html default\n",
    "                  'count':1}\n",
    "\n",
    "##To try and get only the recent filings 4 quarters +1 to decide\n",
    "                  \n",
    "    ## We will save the content that is sent back to us in a response variable:\n",
    "    #  and then we will define the url as the endpoint and set out parameters to the param_dict\n",
    "    response1 = requests.get(url = endpoint, headers = {'user-agent':'UCLA'}, params = param_dict)\n",
    "    print('Request1 Successful')\n",
    "    print(response1.url)\n",
    "    SubComps_Suburls.append(response1.url)\n",
    "    \n",
    "    \n",
    "    ##Parsing that response content and specify the parser - we know it is going to be html that comes back:\n",
    "    soup1 = BeautifulSoup(response1.content,'html.parser')\n",
    "    # find the document table with our data-- check using inspect how the table of all files is coded--it \n",
    "    #  has class = tablefile2 in the name\n",
    "\n",
    "    doc_table = soup1.find_all('table', class_='tableFile2')\n",
    "    #define a base url that will be used for link building because you do not get the entire thing from href.\n",
    "\n",
    "    base_url_sec = r\"https://www.sec.gov\"\n",
    "\n",
    "\n",
    "    for row in doc_table[0].find_all('tr'):##didn't use placeholder 0 to get the recentmost filing because that's just table headers\n",
    "        \n",
    "        cols = row.find_all('td')\n",
    "\n",
    "        # if there are no columns move onto the next row.\n",
    "        if len(cols)!=0:#otherwise actually go onto grab the text\n",
    "            filing_type = cols[0].text.strip()##strip the text for anything that is not useful\n",
    "            filing_date = cols[3].text.strip()##strip the text for anything that is not useful\n",
    "            filing_numb = cols[4].text.strip()##strip the text for anything that is not useful\n",
    "\n",
    "            ##find the links\n",
    "            #  I am going to look for an href that has the id = documentsbutton\n",
    "            #The first parameter that has to be met is to see that href = true and the id should be 'documentsbutton'--from inspect\n",
    "\n",
    "            filing_doc_href = cols[1].find('a',{'href':True, 'id':'documentsbutton'})\n",
    "            filing_interactive_link = cols[1].find('a',{'href':True, 'id':'interactiveDataBtn'}) \n",
    "            ##In the proxy DEF 14A, filing_doc_href is not there, one has to click documents href and then go to the page with\n",
    "            filing_num_href = cols[4].find('a')\n",
    "            ##@18:37\n",
    "\n",
    "        ## To actually get to the proxy statement, we need to click the filing_doc_href\n",
    "            ##Grabbing the first href through the document button\n",
    "            if filing_doc_href != None:\n",
    "                filing_doc_link =base_url_sec+filing_doc_href['href']\n",
    "            else:\n",
    "                filing_doc_link = 'no link'\n",
    "\n",
    "            ##Grabbing the last number href\n",
    "            if filing_num_href != None:\n",
    "                filing_num_link =base_url_sec+filing_num_href['href']\n",
    "            else:\n",
    "                filing_num_link = 'no link'\n",
    "                \n",
    "                \n",
    "            ## This part for going to interactive data\n",
    "            ##Grabbing the first href through the document button\n",
    "            if filing_interactive_link != None:\n",
    "                filing_table_link =base_url_sec+filing_interactive_link['href']\n",
    "            else:\n",
    "                filing_table_link = 'no link'\n",
    "\n",
    "\n",
    "            ## Create and store parsed data in the dictionary\n",
    "            # creating a dictionary and then setting a key where the key is equal to the value we parsed above\n",
    "\n",
    "            file_dict = {}\n",
    "            file_dict['file_type'] = filing_type\n",
    "            file_dict['file_number'] = filing_numb\n",
    "            file_dict['file_date'] = filing_date\n",
    "            file_dict['links'] = {}\n",
    "            file_dict['links']['documents'] = filing_doc_link\n",
    "            file_dict['links']['Interactive Data'] = filing_table_link\n",
    "            file_dict['links']['filing_number'] = filing_num_link\n",
    "             ##To extract only the recentmost filing details i.e. 2021 for now:\n",
    "\n",
    "            if '2021' in filing_date:#####<<<<<<<<<<< if it contains year 2022\n",
    "            \n",
    "                # let the user know it's working\n",
    "                print('-'*100)        \n",
    "                print(\"Filing Type: \" + filing_type)\n",
    "                print(\"Filing Date: \" + filing_date)\n",
    "                print(\"Filing Number: \" + filing_numb)\n",
    "                print(\"Document Link: \" + filing_doc_link)\n",
    "                print(\"Filing Number Link: \" + filing_num_link)\n",
    "                print(\"Interactive Data Link: \" + filing_table_link)\n",
    "                \n",
    "\n",
    "\n",
    "                # append dictionary to master list\n",
    "                \n",
    "                file_dict1 = {}\n",
    "                file_dict1['file_type'] = filing_type\n",
    "                file_dict1['file_number'] = filing_numb\n",
    "                file_dict1['file_date'] = filing_date\n",
    "                file_dict1['links'] = {}\n",
    "                file_dict1['links']['documents'] = filing_doc_link\n",
    "                file_dict1['links']['Interactive Data'] = filing_table_link\n",
    "                file_dict1['links']['filing_number'] = filing_num_link\n",
    "                master_links.append(file_dict1)\n",
    "                \n",
    "                ##To avoid error because of Sweetgreen not having a filing as a recent company:\n",
    "                if filing_table_link!='no link':\n",
    "                    \n",
    "                    response2 = requests.get(url = filing_table_link, headers = {'user-agent':'UCLA'})##going to the recent-most document DEF14A link\n",
    "\n",
    "                    print('Request2 Successful')\n",
    "                    print(response2.url)\n",
    "                    print('Link to Tables')\n",
    "                    print(\"/\".join(filing_doc_link.split(\"/\")[:8])+\"/\"+\"index.json\")##keeping the CIK and Accession number and removing everything after the 8th /\n",
    "\n",
    "                    ##Parsing that response content and specify the parser - we know it is going to be html that comes back:\n",
    "                    soup2 = BeautifulSoup(response2.content,'html.parser')\n",
    "                    response2.content\n",
    "\n",
    "                    SecondResponses.append(response2.content)\n",
    "                    AllSoup2.append(soup2)\n",
    "\n",
    "                    # soup2.prettify()\n",
    "\n",
    "    #                 AllFilings = soup2.find_all('li', class_ = 'accordion')[0].find_all('a', class_=\"xbrlviewer\")\n",
    "\n",
    "                    AllFilings = soup2.find_all('li', class_ = 'accordion')[0].find_all('a', class_=\"xbrlviewer\")\n",
    "                    \n",
    "                    Report_Name=[]\n",
    "\n",
    "                    for pos in list(range(1,len(AllFilings),1)):\n",
    "                        FilingName = AllFilings[pos].text.strip()\n",
    "                        Report_Name.append(FilingName)\n",
    "\n",
    "                    #Report_Name#[0:6]\n",
    "\n",
    "                    ###### next\n",
    "\n",
    "\n",
    "                    documents_url=\"/\".join(filing_doc_link.split(\"/\")[:8])+\"/\"+\"index.json\" ##Apple 10K\n",
    "\n",
    "                                    ##Without the index.json, view the site and has a lot of files that are basically each just a table! \n",
    "                                    # For example, see https://www.sec.gov/Archives/edgar/data/1725516/000149315221032816/R2.htm\n",
    "                                    #request the url and decode it.\n",
    "                    headers = {'user-agent':'UCLA'}\n",
    "                                    #headers = {'user-agent': 'sample text'}\n",
    "\n",
    "                    content = requests.get(documents_url, headers=headers).json()\n",
    "                                    ##Then loop through the json dictionary and go to the directory\n",
    "                    for file in content['directory']['item']:\n",
    "\n",
    "                                    #Grab the filing summary xml and create a new url leading to the file so we can download it.\n",
    "                        if file['name']=='FilingSummary.xml':#Then we will create a path\n",
    "                                        #https://note.nkmk.me/en/python-str-compare/\n",
    "                                        #if re.search('.xml', file['name']):#trying to look for .xml instead\n",
    "                            xml_summary = base_url_sec+content['directory']['name']+'/'+ file['name']\n",
    "                                            ##name key returns the url except base_url\n",
    "                            print('-'*100)\n",
    "                            print('File name: ' + file['name'])\n",
    "                            print('File Path: ' + xml_summary)\n",
    "                                            #@9:36\n",
    "\n",
    "                        content\n",
    "                                # define a new base url that represents the filing folder. This will come in handy when we need to download the reports.\n",
    "                    base_url = xml_summary.replace('FilingSummary.xml','')\n",
    "                            #base_url # coming to the directory list by removing last part of the link to land at the place where each report is placed together\n",
    "\n",
    "                            # request and parse the content\n",
    "                    content = requests.get(xml_summary, headers = headers).content\n",
    "                            #content ## the dot content will list out the nestedness b'<?xml version=\\'1.0\\' encoding=\\'utf-8\\'?>\\n<FilingSummary>\\n  <Version>3.19.1</Version>\\n  <ProcessingTime/>\\n  <ReportFormat>html</ReportFormat>\\n  <ContextCount>224</ContextCount>\\n  <ElementCount>324</ElementCount>\\n  <EntityCount>1</EntityCount>\\n  <FootnotesReported>false</FootnotesReported>\\n  <SegmentCount>73</SegmentCount>\\n  <ScenarioCount>0</ScenarioCount>\\n  <TuplesReported>false</TuplesReported>\\n  <UnitCount>7</UnitCount>\\n  <MyReports>\\n    <Report instance=\"mtii-20181231.xml\">\\n      <IsDefault>false</IsDefault>\\n      <HasEmbeddedReports>false</HasEmbeddedReports>\\n      <HtmlFileName>R1.htm</HtmlFileName>\\n      <LongName>0001000 - Document - Document and Entity Information</LongName>\\n      <ReportType>Sheet</ReportType>\\n      <Role>http://www.monitronics.com/role/DocumentAndEntityInformation</Role>\\n      <ShortName>Document and Entity Information</ShortName>\\n      <MenuCategory>Cover</MenuCategory>\\n      <Position>1</Position>\\n    </Report>\\n    <Report instance=\"mtii-20181231.xml\">\\n      <IsDefault>false</IsDefault>\\n      <HasEmbeddedReports>false</HasEmbeddedReports>\\n      <HtmlFileName>R2.htm</HtmlFileName>\\n      <LongName>1001000 - Statement - Consolidated Balance Sheets</LongName>\\n      <ReportType>Sheet</ReportType>\\n      <Role>http://www.monitronics.com/role/ConsolidatedBalanceSheets</Role>\\n      <ShortName>Consolidated Balance Sheets</ShortName>\\n      <MenuCategory>Statements</MenuCategory>\\n      <Position>2</Position>\\n    </Report>\\n    <Report instance=\"mtii-20181231.xml\">\\n      <IsDefault>false</IsDefault>\\n      <HasEmbeddedReports>false</HasEmbeddedReports>\\n      <HtmlFileName>R3.htm</HtmlFileName>\\n      <LongName>1001501 - Statement - Consolidated Balance Sheets (Parenthetical)</LongName>\\n      <ReportType>Sheet</ReportType>\\n      <Role>http://www.monitronics.com/role/ConsolidatedBalanceSheetsParenthetical</Role>\\n      <ShortName>Consolidated Balance Sheets (Parenthetical)</ShortName>\\n      <MenuCategory>Statements</MenuCategory>\\n      <Position>3</Position>\\n    </Report>\\n    <Report instance=\"mtii-20181231.xml\">\\n      <IsDefault>false</IsDefault>\\n      <HasEmbeddedReports>false</HasEmbeddedReports>\\n   \n",
    "\n",
    "                    soup = BeautifulSoup(content, 'lxml')\n",
    "                            #soup ##Beautifying the nestedness for easier reading so that \\n is a different line so its closer to how it looks on https://www.sec.gov/Archives/edgar/data/1265107/000126510719000004/FilingSummary.xml\n",
    "\n",
    "                            ##Then, find the 'MyReports' tag because it contains all the individual reports submitted\n",
    "                    reports = soup.find('myreports')\n",
    "                            # reports: all filed reports under the tag my reports\n",
    "\n",
    "                                    ## I want a list to store all the individual components of the report, so creating the master list:\n",
    "                    master_reports = []\n",
    "\n",
    "                        \n",
    "                    for report in reports.find_all('report')[:-1]:\n",
    "\n",
    "                                # Let's create an empty dictionary - curly brackets\n",
    "                        report_dict = {}\n",
    "                                ##then assign new keys to it and each value assigned to the keys is a different portion of the report tag\n",
    "                        report_dict['name_short'] = report.shortname.text ##get only the text from the tag ShortName\n",
    "                        report_dict['name_long'] = report.longname.text ##get only the text from the tag ShortName\n",
    "                        #report_dict['position'] = report.position.text ##get only the text from the tag ShortName\n",
    "                        report_dict['category'] = report.menucategory.text ##get only the text from the tag ShortName\n",
    "                        report_dict['url'] = base_url + report.htmlfilename.text ##get only the text from the tag ShortName\n",
    "\n",
    "                                ## We need to append this to the master report we created outside the for loop\n",
    "                        master_reports.append(report_dict)\n",
    "\n",
    "                         \n",
    "                    for report in master_reports:\n",
    "\n",
    "                                #Define the statements we want to look for\n",
    "                        item1 = r\"Consolidated Balance Sheets\" ##r' means raw string\n",
    "                        item2 = r\"Consolidated Statements of Operations and Comprehensive Income (Loss)\"\n",
    "                        item3 = r\"Consolidated Statements of Cash Flows\"\n",
    "                        item4 = r\"Consolidated Statements of Stockholder's (Deficit) Equity\"\n",
    "                        item5 = r\"Property and Equipment\"##Capital Expenditure\n",
    "                        item6 = r\"Consolidated Statements of Income\"\n",
    "                        item7 = r\"Income Taxes\"\n",
    "                        item8 = r\"Property and Equipment\"\n",
    "\n",
    "                           \n",
    "\n",
    "                                #storing them in a list\n",
    "                        titles = [item1.lower(), item2.lower(), item3.lower(), item4.lower(), item5.lower(), item6.lower(),\n",
    "                                  item7.lower(),item8.lower()] ## + means concatenate and comma means or\n",
    "\n",
    "                        keywords = [\"cover\",\"balance sheets\", \"of operations\",\"of earning\",\n",
    "                                    \"cash flow\",\"income tax\",\"property and equipment\",\"statement\",\"statements\"]\n",
    "\n",
    "                        test_string = report['name_short'].lower()\n",
    "\n",
    "                        # initializing test list\n",
    "                        test_list = keywords\n",
    "                        \n",
    "\n",
    "                        flag=0\n",
    "                        #for i in test_string:\n",
    "                        for j in test_list:\n",
    "                            if j in test_string and \"quarterly\" not in test_string:\n",
    "                                #and \"(\" not in test_string is leading to removal of statements that have (loss) in the title\n",
    "                                        #if all(x not in i for x in mylist):\n",
    "                                flag=1\n",
    "                                break#https://www.geeksforgeeks.org/python-test-if-string-contains-element-from-list/\n",
    "                        if flag==1:\n",
    "                                    #print(\"String contains the list element\")\n",
    "                                                    ## Print some info and store it in the statements_url\n",
    "                            print('-'*100)\n",
    "                            print(report['name_short'])\n",
    "                            print(report['url'])\n",
    "\n",
    "    #                         statements_url.append(report['url'])\n",
    "    #                         statements_url.append(,ignore_index=True)\n",
    "                            url_df = pd.DataFrame({'cik_str':[relevantCIK],'URL':[report['url']], 'TableName':[report['name_short']],'LinkToAllTabs':response2.url})\n",
    "                            statements_urlDF.append(url_df)\n",
    "                            statements_url.append(report['url'])\n",
    "\n",
    "                            #     else:\n",
    "                            #         print(\"String does not contain the list element\")\n",
    "\n",
    "\n",
    "                        #statements_url\n",
    "\n",
    "time_taken = (time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(pd.concat(statements_url))\n",
    "type(statements_url[0])\n",
    "statements_url\n",
    "pd.DataFrame (statements_url, columns = ['URL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exporting list of dataframes to csv for later use/import\n",
    "\n",
    "pd.concat(statements_urlDF).to_csv('C:/UCLAAnderson/10KfilingsTables/Data/AutomatedFinancialData/July13_SP503WikiCompanies_statements_urlDF.csv')\n",
    "pd.DataFrame (statements_url, columns = ['URL']).to_csv('C:/UCLAAnderson/10KfilingsTables/Data/AutomatedFinancialData/July13_SP503WikiCompanies_statements_url.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##Maybe include a column for the link to request 2 so that NA's in tables can be cross-checked\n",
    "\n",
    "# statements_list2df = pd.concat(statements_urlDF)##to convert list of pandas dataframes to a dataframe\n",
    "# ##merge with the Companies dataframe to get company names and TIKR\n",
    "# CompanyInfo = pd.DataFrame(Companies, columns = ['cik_str','ticker','title','ForCrossCheckingNAtables'])\n",
    "\n",
    "# ##merge the two dataframes by CIK:\n",
    "# Company10KfilingsURL = pd.merge(CompanyInfo, statements_list2df, on ='cik_str')\n",
    "# ##Export this metadata\n",
    "\n",
    "# Company10KfilingsURL.to_csv('C:/UCLAAnderson/10KfilingsTables/July6_22MissingCompanies_before2021_ShortenedCompanyMetadataAnd10KTableURLs.csv', index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'110825' in statements_url\n",
    "type(statements_url)\n",
    "matching = [s for s in statements_url if \"1108524\" in s]\n",
    "matching\n",
    "# len(statements_data)\n",
    "# iterate through list of elements\n",
    "# for i in statements_url:\n",
    "   \n",
    "#     # check for type is str\n",
    "#     if(type(i) is str):\n",
    "       \n",
    "#         # display index\n",
    "#         print(statements_url.index(i))\n",
    "###finding the location/index where salesforce links are stored in this list ##https://stackoverflow.com/questions/2170900/get-first-list-index-containing-sub-string\n",
    "index = [idx for idx, s in enumerate(statements_url) if '1108524' in s][0]\n",
    "index\n",
    "\n",
    "statements_url[7897]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRIAL FOR GETTING $MILLIONS UNITS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TRYING TO CLEAN DATA ALONGSIDE\n",
    "\n",
    "import re\n",
    "##Finally the correct code to get three dataframes separately for all companies\n",
    "##Remaining pseudocoding for headers in income tax as they vary from one date, two dates to three dates\n",
    "\n",
    "\n",
    "##useful functions:\n",
    "\n",
    "## Let's assume we want all the statements in a single dataset.\n",
    "# del each\n",
    "start_AllTables=time.time()\n",
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "# list(range(len(statements_data)))\n",
    "\n",
    "def concat(*args):\n",
    "    return pd.concat([x for x in args if not x.empty])\n",
    "\n",
    "##Empty lists\n",
    "\n",
    "statements_data = [] ## empty master list with all the statements\n",
    "AllTablesOfACompany=[]\n",
    "Allstatements_report_soup=[]\n",
    "# ##Start with empty dataframes:\n",
    "# Company10KData=pd.DataFrame(columns = [0, 1, 2, 3, 'Company', 'cik_str'])\n",
    "# OperationsStatement=pd.DataFrame(columns = [0, 1, 2, 3, 'Company', 'cik_str'])\n",
    "# TotalUStax=pd.DataFrame(columns = [0, 1, 2, 3, 'Company', 'cik_str'])\n",
    "# ResearchDev=pd.DataFrame(columns = [0, 1, 2, 3, 'Company', 'cik_str'])\n",
    "\n",
    "\n",
    "# Loop through each statement url:\n",
    "for statement in statements_url:\n",
    "    \n",
    "    #define a dictionary that will store the different parts of the statement\n",
    "    ##After inspecting (after right clicking the table in https://www.sec.gov/Archives/edgar/data/1265107/000126510719000004/R2.htm)\n",
    "    ## I find that I expect to have different sections of my financial statement\n",
    "    ## so I just store each section and the corresponding component of the actual statement data dictionary\n",
    "    \n",
    "    statement_data = {} # empty dictionary\n",
    "    statement_data['headers']=[] #empty list\n",
    "    statement_data['sections'] = []\n",
    "    statement_data['data'] = [] #actual numbers\n",
    "    \n",
    "    headers = {'user-agent':'UCLA'}\n",
    "    #Request the statement file content\n",
    "    content = requests.get(statement, headers = headers).content #get content of the requested url\n",
    "    \n",
    "    # Create a new variable for parsing\n",
    "    report_soup = BeautifulSoup(content, 'html') # In this example the parser is HTML\n",
    "    Allstatements_report_soup.append(report_soup)\n",
    "    \n",
    "    \n",
    "    for index, row in enumerate(report_soup.table.find_all('tr')): \n",
    "    #----> tr is how all the rows are stated inside table when inspected\n",
    "    ##Again look in the drop-down of tr row you see 'td' which is basically individual element or each column \n",
    "    # We will basically find all the elements of any given row and store it in anew variable called columns\n",
    "    \n",
    "        cols = row.find_all('td')\n",
    "    \n",
    "    \n",
    "    # So,creating a logic statement with this observation\n",
    "    # if its a regular row snd not a section or a table header\n",
    "    \n",
    "        if(len(row.find_all('th'))==0 and len(row.find_all('strong'))==0):\n",
    "            # next we are going to declare a new variable and then do list comprehension to parse each individual element\n",
    "            # for element in column as defined above cols, we will say element.text.strip \n",
    "            # for each element \n",
    "            \n",
    "            reg_row = [ele.text.strip() for ele in cols] #- this will be stored in a list\n",
    "            statement_data['data'].append(reg_row)\n",
    "            \n",
    "        ## if a section row:\n",
    "        elif(len(row.find_all('th'))==0 and len(row.find_all('strong'))!=0):\n",
    "            \n",
    "            sec_row = cols[0].text.strip() ### all I really want in a section row is the name of the section\n",
    "            statement_data['sections'].append(sec_row) ## Appending it to the sections part of the master statement\n",
    "            \n",
    "            ## if a header row\n",
    "        elif(len(row.find_all('th'))!=0): #because some table headers may or may not have a strong tag\n",
    "            hed_row = [ele.text.strip() for ele in row.find_all('th')] #- instead of looking across column, now looking across rows for the category names\n",
    "            statement_data['headers'].append(hed_row)\n",
    "            \n",
    "        else:\n",
    "            print('We encountered an error.')\n",
    "\n",
    "            \n",
    "            ##We finally need to append this to the master list\n",
    "    statements_data.append(statement_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statements_data_DF=pd.DataFrame(statements_data)\n",
    "\n",
    "statements_data_DF.to_csv('C:/UCLAAnderson/10KfilingsTables/Data/AutomatedFinancialData/ForReadingIn/July13_SP503WikiCompanies_Statements_data_DF.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# statements_dataOut = pd.read_csv('C:/UCLAAnderson/10KfilingsTables/Data/AutomatedFinancialData/ForReadingIn/April13_Statements_data_DF.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_headers=[]\n",
    "for each1 in list(range(0,len(statements_data))):\n",
    "    print(statements_data[each1]['headers'][0][0])\n",
    "    print(each1)\n",
    "    if '$ in' in statements_data[each1]['headers'][0][0].lower():\n",
    "        unit_headers.append(statements_data[each1]['headers'][0][0].lower().split(\"$ in\",1)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statements_data[40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listNum(myList, x):\n",
    "    return [item for item in myList if type(item) is str and len(item) == x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('July13_SP503WikiUniqueUnitsUsed.csv', 'w',encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    for url in set(unit_headers):\n",
    "        writer.writerow([url])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the statements_data list of dictionaries because it takes a long time to run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing statements_data saved earlier for 2021 to avoid spending hours in running the previous code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(statements_data)\n",
    "statements_data\n",
    "statements_urlDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# worklist = list(range(0,len(statements_data),1))\n",
    "# batchsize = 500\n",
    "\n",
    "# for each in range(0, len(worklist), batchsize):\n",
    "#     batch = worklist[each:each+batchsize]\n",
    "# #     print(batch)\n",
    "    \n",
    "#     for each in batch:\n",
    "#         print(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(statements_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original with single loop####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install -c conda-forge nbstripout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Start with empty dataframes:\n",
    "\n",
    "DFOperationsStatement1=pd.DataFrame()\n",
    "OuterDFOperationsStatement=pd.DataFrame()\n",
    "DF_RandD=pd.DataFrame()\n",
    "DFTaxStats=pd.DataFrame()\n",
    "\n",
    "##and with empty lists:\n",
    "List_RandD=[]\n",
    "ListTaxStats=[]\n",
    "Company10KData=[]\n",
    "CompanyFinancials=[]\n",
    "OperationsStatement=[]\n",
    "TotalUStax=[]\n",
    "ResearchDev=[]\n",
    "RandD=[]\n",
    "TaxnOperationsStatement=[]\n",
    "TaxOperationsRnD_Statement=[]\n",
    "# Ops_sel=[]\n",
    "Ops_sel=pd.DataFrame()\n",
    "NetRevenue=[]\n",
    "NetSales=[]\n",
    "OperatingCost=[]\n",
    "EmployeeWages=[]\n",
    "df1B=pd.DataFrame()\n",
    "df_out1B=pd.DataFrame()\n",
    "df_Revenue=pd.DataFrame()\n",
    "\n",
    "df_out1B=pd.DataFrame()\n",
    "\n",
    "##Iterating over a list of dictionaries\n",
    "# for each in list(range(0,len(statements_data),1)):#     for key in statements_data[index]:\n",
    "# worklist = list(range(0,len(statements_data),1))\n",
    "# batchsize = 500\n",
    "\n",
    "# for Outereach in range(0, len(worklist), batchsize):\n",
    "#     batch = worklist[Outereach:Outereach+batchsize] # the result might be shorter than batchsize at the end\n",
    "    # do stuff with batch\n",
    "    ## Grabbing the proper components\n",
    "    ## headers and data are lists of lists so making it a list instead\n",
    "# for each in list(range(0,3,1)):\n",
    "# for each in list(range(0,len(statements_data),1)):#batch:\n",
    "# for each in list(range(0,round(len(statements_data)/2),1)):#batch:\n",
    "for each in list(range(round(len(statements_data)/2),len(statements_data),1)):#batch:\n",
    "\n",
    "    income_headers = statements_data[each]['headers']##for taking the dates stored as the second list in headers of \n",
    "                                                     ##Consolidated Statements of Operations and Comprehensive Income (Loss) - USD ($) $ in Thousands\n",
    "\n",
    "        ##The balance sheet has only one header but if you go to the income statement it has two headers \n",
    "        ##we take [1] because we were interested in the dates header which was the second element\n",
    "\n",
    "    income_data = statements_data[each]['data']\n",
    "    # income_data ##here we have each row and the corresponding value for each particular financial statement- different sections\n",
    "\n",
    "\n",
    "    ## Put the data in a dataframe\n",
    "    income_df = pd.DataFrame(income_data)\n",
    "\n",
    "    ##For displaying\n",
    "\n",
    "#     print('-'*100)\n",
    "#     print('Before Reindexing')\n",
    "#     print('-'*100)\n",
    "#     display(income_df.head(7)) ## here we see the problem\n",
    "    ## first problem is that index is actually the Net Revenue, cost of services etc....\n",
    "\n",
    "    ## So, define the index column, rename it, and we need to make sure to drop the old column once we reindex.\n",
    "\n",
    "    income_df.index = income_df[0]##column 0 is first column-- something like row.names in R\n",
    "\n",
    "\n",
    "\n",
    "    ## Sometimes before removing the first column to row.names instead of replacing you can also assign it a new intuitive name\n",
    "\n",
    "    income_df.index.name = 'Category'\n",
    "    ##now dropping the first column which is now a rowname or index:\n",
    "    income_df = income_df.drop(0, axis = 1) ##drop method takes the column number to be dropped and along which axis \n",
    "                                            ##- 1 means along column axis\n",
    "\n",
    "\n",
    "    ## Now, we see problem 2, which is the dollar signs -  which tells me that right now it is a string\n",
    "    ## this being a string is a problem because I can't do any calculations on a string\n",
    "    ## Additionally, the blank values should be represented as a non-value (Problem 3)\n",
    "    ## 4th problem: negative values in losses should be true negatives and not have $\n",
    "\n",
    "    ## So, running some replace operations on this dataframe\n",
    "\n",
    "    ## Get rid of the '$', '(', ')', and convert the '' to NaNs. @44:34\n",
    "    #dealing with replacing positive values\n",
    "    #then, dealing with negative values such that if you see closing bracket replace with minus sign\n",
    "    # finally, if you see empty cells, replace with NaNs non values\n",
    "    income_df = income_df.replace('[\\$,)]','', regex = True)\\\n",
    "    .replace('[(]','-', regex = True)\\\n",
    "    .replace('',np.nan, regex = True)\n",
    "\n",
    "\n",
    "    CompNameCIK=str()\n",
    "    NetRevenue=[]\n",
    "    NetSales=[]\n",
    "    OperatingCost=[]\n",
    "    EmployeeWages=[]\n",
    "#         OperatingCost=pd.DataFrame()\n",
    "#         Ops_sel=pd.DataFrame()\n",
    "#         ops=pd.DataFrame()\n",
    "    ##Now that we aren't relying on the cover page to get the name of \n",
    "\n",
    "    soupy=BeautifulSoup(requests.get(statements_urlDF[each]['LinkToAllTabs'][0], headers = headers).content,'html.parser')\n",
    "\n",
    "    node = soupy.find('span', class_='companyName')\n",
    "    if node is not None:\n",
    "        ementa = node.text\n",
    "        CompNameCIK=soupy.find('span', class_='companyName').text.strip().split('CIK:')\n",
    "        CompanyName = CompNameCIK[0]\n",
    "        CIK = CompNameCIK[1]\n",
    "#     else:\n",
    "#         ementa = None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if 'operation' in statements_data[each]['headers'][0][0].lower() or 'income' in statements_data[each]['headers'][0][0].lower() or 'earning' in statements_data[each]['headers'][0][0].lower():\n",
    "#         or 'cash flow' in statements_data[each]['headers'][0][0].lower():\n",
    "#     or 'consolidated' in statements_data[each]['headers'][0][0].lower() or 'statement' in statements_data[each]['headers'][0][0].lower() or 'sheet' in statements_data[each]['headers'][0][0].lower():\n",
    "\n",
    "\n",
    "\n",
    "####################################################################################################################   \n",
    "\n",
    "\n",
    "\n",
    "        NetRevenue = pd.DataFrame(statements_data[each]['data']).loc[pd.DataFrame(statements_data[each]['data']).iloc[:,0].apply(lambda x: x.lower()).str.contains('|'.join(['revenue','revenues',\n",
    "        'sale','sales','net loss','net (loss)',\n",
    "        'net interest income','total operating','interest income','net interest',\n",
    "#         'net cash','earning','net other',\n",
    "#         'asset','liabilit',\n",
    "#         'general and administrative','general & administrative',#selling, \n",
    "        'research and development','research & development', 'R&D','R & D',\n",
    "#         'effective tax rate','effective rate'\n",
    "])),:]\n",
    "\n",
    "#######Using \"in\" instead of str.contains:\n",
    "#         stat_df=pd.DataFrame(statements_data[each]['data'])\n",
    "#         NetRevenue = stat_df[stat_df[0].apply(lambda x: any(term in x.lower() for term in ['revenues', 'revenue','sale','sales',\n",
    "#                                                                                  'net loss','net (loss)','net interest income',\n",
    "#                                                                                  'total operating','interest income','net interest',\n",
    "#                                                                                            'research and development','research & development', 'R&D','R & D']))]\n",
    "\n",
    "#         df1=NetRevenue.replace(\"\\$\",'',regex=True)\n",
    "\n",
    "############## ********** Make sure to add backward slash to avoid replacing the values 1 and 2 occuring in revenue values\n",
    "        df = NetRevenue[NetRevenue.columns].replace({'\\(':'-','\\)':'','':'','%':'','\\$':'',' ':'',',':'','\\[1]':'','\\[2]':''}, regex = True)\n",
    "    # using apply method\n",
    "        cols = df.columns.drop(df.columns[0])\n",
    "        df[cols]=df[cols].apply(lambda x: pd.to_numeric(x, errors='coerce'))####Converting non metric columns to numeric to be able to multiply later with\n",
    "\n",
    "\n",
    "        if len(df)!=0:\n",
    "            df_Revenue=df\n",
    "\n",
    "            if '$ in millions' in statements_data[each]['headers'][0][0].lower():# and '$' in NetRevenue:\n",
    "               #and NetRevenue>1000  \n",
    "            ##Cleaning the data before building it:\n",
    "                s =df_Revenue[df_Revenue.select_dtypes(include=['number']).columns]*1000000\n",
    "                df[s.columns] = s\n",
    "            elif '$ in thousands' in statements_data[each]['headers'][0][0].lower():# and '$' in NetRevenue:\n",
    "                s =df_Revenue[df_Revenue.select_dtypes(include=['number']).columns]*1000\n",
    "                df[s.columns] = s\n",
    "            elif '$ in billions' in statements_data[each]['headers'][0][0].lower():# and '$' in NetRevenue:\n",
    "                s =df_Revenue[df_Revenue.select_dtypes(include=['number']).columns]*1000000000\n",
    "                df[s.columns] = s\n",
    "                ## below is an exception\n",
    "    #         elif '$ in millions, $ in billions' in statements_data[each]['headers'][0][0].lower():# and '$' in NetRevenue:\n",
    "    #             # and NetRevenue<1000 \n",
    "    #             df_Revenue =df_Revenue[df_Revenue.select_dtypes(include=['number']).columns]*1000000000\n",
    "            else:\n",
    "                s=df_Revenue[df_Revenue.select_dtypes(include=['number']).columns]*1\n",
    "                df[s.columns] = s\n",
    "\n",
    "            df1B_duped = pd.concat([df[df.columns[0]], df_Revenue], axis=1)\n",
    "\n",
    "\n",
    "            ####Dedupe column names right here to change colnames ############\n",
    "\n",
    "\n",
    "            df1B_dedupe = df1B_duped.loc[:,~df1B_duped.columns.duplicated()].copy()\n",
    "\n",
    "            ##for later\n",
    "            df1B=df1B_dedupe.dropna(axis=0)\n",
    "    ## Below is to avoid reindex error due to repeated columns like deferred tax revenue being reported in two columns acc to: https://stackoverflow.com/questions/14984119/python-pandas-remove-duplicate-columns\n",
    "    #         df1B = pd.concat([df[df.columns[0]], df_Revenue], axis=1).loc[:,~(pd.concat([df[df.columns[0]], df_Revenue], axis=1)).columns.duplicated()].copy()\n",
    "\n",
    "    #             df1B = df_Rev[df_Rev.columns].replace({'\\(':'-','\\)':'','':'','%':'','\\$':'',' ':'',',':''}, regex = True)\n",
    "        #         '\\$':''<<-removed this condition because if there is $ in front of the value, the headers direct the units thousand or million\n",
    "\n",
    "                #         if '$'in df1B:\n",
    "                    #multiply bymillion or thousand\n",
    "#             df_out1B=pd.DataFrame()\n",
    "            ##2. Replace NanNs with next column's non-NaN value\n",
    "            if df1B.isnull().values.any():\n",
    "                df_out1B = df1B.fillna(method='ffill',axis=0).dropna(axis=1)\n",
    "    #                 df_out1B = df1B.apply(lambda x: pd.Series(x.dropna().to_numpy()), axis=1)\n",
    "    #                 df_out1B = df_out1B.set_axis(df1B.columns[:df_out1B.shape[1]], axis=1).reindex(df1B.columns, axis=1).dropna(axis=1)\n",
    "            else:\n",
    "                df_out1B = df1B.dropna(axis=1)\n",
    "            print(df_out1B)\n",
    "\n",
    "            ##3. replacing empty whitespace with NaN---do we need to do this again?\n",
    "            cleaned_NetRevenue=df_out1B.replace(r'^\\s*$', np.nan, regex=True).dropna(axis=1, how='all')\n",
    "\n",
    "\n",
    "            if len(CompNameCIK)!=0:\n",
    "                cleaned_NetRevenue['Company']= CompanyName\n",
    "                cleaned_NetRevenue['cik_str']= CIK\n",
    "            else:\n",
    "                cleaned_NetRevenue['Company']= 'NotFound'\n",
    "                cleaned_NetRevenue['cik_str']= 'NotFound'\n",
    "            print(cleaned_NetRevenue) \n",
    "\n",
    "    #######################To avoid the initial columns being 0,1,2,3 followed with opsheader dates##############\n",
    "\n",
    "\n",
    "    ##################################################################################\n",
    "        ##To avoid clumping all dates into a single list:\n",
    "            if len(statements_data[each]['headers'])>1:\n",
    "                        dates_init = statements_data[each]['headers'][1]\n",
    "            else:\n",
    "                dates_init = statements_data[each]['headers'][0]\n",
    "\n",
    "\n",
    "            dateslist_initial=[]\n",
    "            for onedate in dates_init:\n",
    "            #     print(re.sub(r'(,?!(([^\"]*\"){2})*[^\"]*$)',':', str(onedate)))\n",
    "                mystring1 = re.sub(r'(,?!(([^\"]*\"){2})*[^\"]*$)',':', str(onedate))\n",
    "                print(re.sub(\",\", \"/\", mystring1))\n",
    "                datestr1a=re.sub(\",\", \"/\", mystring1)## replace comma with / before the year\n",
    "                if len(datestr1a.split(\"/\"))>1:\n",
    "                    datestr1=datestr1a.split(\"/\")[0].split(\" \")[0]+\" \"+datestr1a.split(\"/\")[1]\n",
    "                    ##To drop the exact date, splitting at \\ for year and then splitting at space to keep Month\n",
    "                else:\n",
    "                    datestr1=datestr1a.split(\"/\")[0].split(\" \")[0]+\" \"+datestr1a.split(\"/\")[0]\n",
    "                dateslist_initial.append(datestr1)\n",
    "\n",
    "            print(dateslist_initial)\n",
    "            singledatesstring1 = ','.join(dateslist_initial)\n",
    "            datesList1_init = re.sub(\",\", \",\", singledatesstring1).split(',')\n",
    "\n",
    "            CleanRevHeaders = ['Metric']\n",
    "\n",
    "\n",
    "            for each1 in datesList1_init:\n",
    "                print('-'.join(each1.split(',')))\n",
    "                CleanRevHeaders.append(''.join(each1.split(',')))\n",
    "\n",
    "\n",
    "            import itertools\n",
    "            AllCleanRevHeaders = list(itertools.chain(CleanRevHeaders, ['Company'], ['cik_str']))\n",
    "\n",
    "            if len(cleaned_NetRevenue.columns.tolist())==len(AllCleanRevHeaders):\n",
    "                cleaned_NetRevenue.columns=AllCleanRevHeaders\n",
    "\n",
    "            print(cleaned_NetRevenue)\n",
    "        ##################################################################################\n",
    "\n",
    "    ####################################################################################################################\n",
    "\n",
    "\n",
    "    ##Some tables might not be there like R&D for some companies:\n",
    "            if (len(cleaned_NetRevenue))!=0:#+len(cleaned_NetSales)+len(cleaned_OperatingCost)+len(cleaned_EmployeeWages)+len(cleaned_RandDOpsAmgen))!=0:\n",
    "                #ops = concat(*(cleaned_NetRevenue))#, cleaned_NetSales,cleaned_OperatingCost, cleaned_EmployeeWages,cleaned_RandDOpsAmgen))\n",
    "                ops=cleaned_NetRevenue\n",
    "                if len(ops)!=0:\n",
    "\n",
    "                    editedOps = pd.DataFrame(ops[ops.columns[ops.notnull().all()]].iloc[0].dropna())\n",
    "                    editedOps.dropna(inplace=True)\n",
    "                    editedOps\n",
    "                    # remove empty column that just takes up space and shifts the 3 years' data one cell further\n",
    "                    CleanOps = editedOps.loc[:, editedOps.notna().any()]\n",
    "                    Operations_edit = pd.DataFrame(CleanOps.replace(r'^\\s*$%', np.nan, regex=True).dropna())\n",
    "                    #TotalUStax_edit.where(~TotalUStax_edit.apply(lambda x: x.astype(str).str.contains('|'.join(['%','$'])))).dropna().T\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #             Ops_sel = [ops]\n",
    "                    Ops_sel = Operations_edit.T\n",
    "        #             dates = statements_data[each]['headers'][0]\n",
    "                    if len(statements_data[each]['headers'])>1:\n",
    "                        dates = statements_data[each]['headers'][1]\n",
    "                    else:\n",
    "                        dates = statements_data[each]['headers'][0]\n",
    "\n",
    "    #                     Ops_sel.columns = ['Metric',dates,'Company','cik_str']##<<<<to see if date can be registered\n",
    "\n",
    "\n",
    "        ##################################################################################\n",
    "        ##To avoid clumping all dates into a single list:\n",
    "\n",
    "                    dateslist1=[]\n",
    "                    for onedate in dates:\n",
    "                    #     print(re.sub(r'(,?!(([^\"]*\"){2})*[^\"]*$)',':', str(onedate)))\n",
    "                        mystring1 = re.sub(r'(,?!(([^\"]*\"){2})*[^\"]*$)',':', str(onedate))\n",
    "                        print(re.sub(\",\", \"/\", mystring1))\n",
    "                        datestr1a=re.sub(\",\", \"/\", mystring1)## replace comma with / before the year\n",
    "                        if len(datestr1a.split(\"/\"))>1:\n",
    "                            datestr1=datestr1a.split(\"/\")[0].split(\" \")[0]+\" \"+datestr1a.split(\"/\")[1]\n",
    "                            ##To drop the exact date, splitting at \\ for year and then splitting at space to keep Month\n",
    "                        else:\n",
    "                            datestr1=datestr1a.split(\"/\")[0].split(\" \")[0]+\" \"+datestr1a.split(\"/\")[0]\n",
    "                        dateslist1.append(datestr1)\n",
    "\n",
    "                    print(dateslist1)\n",
    "                    singledatesstring1 = ','.join(dateslist1)\n",
    "                    datesList1 = re.sub(\",\", \",\", singledatesstring1).split(',')\n",
    "\n",
    "                    OpsHeaders = ['Metric']\n",
    "\n",
    "\n",
    "                    for each1 in datesList1:\n",
    "                        print('-'.join(each1.split(',')))\n",
    "                        OpsHeaders.append(''.join(each1.split(',')))\n",
    "\n",
    "\n",
    "                    import itertools\n",
    "                    AllOpsHeaders = list(itertools.chain(OpsHeaders, ['Company'], ['cik_str']))\n",
    "\n",
    "                    if len(Ops_sel.columns.tolist())==len(AllOpsHeaders):\n",
    "                        Ops_sel.columns=AllOpsHeaders\n",
    "        ##################################################################################\n",
    "        #         else:\n",
    "        #             Ops_sel = pd.DataFrame(['NIL'])\n",
    "\n",
    "        ##### Checking for duplicated columns like deferred revenues that create problems in rbind:\n",
    "\n",
    "            Ops_sel_dedupe = Ops_sel.loc[:,~Ops_sel.columns.duplicated()].copy()\n",
    "            OperationsStatement.append(Ops_sel_dedupe)\n",
    "            print(OperationsStatement)\n",
    "\n",
    "\n",
    "            ##Also creating a dataframe instead of a list of dataframes for easier understanding of column values\n",
    "            DFOperationsStatement1 = pd.concat([DFOperationsStatement1, Ops_sel_dedupe])\n",
    "\n",
    "            # Define the filename for the current batch\n",
    "    #             filename = f'batch_{each+1}_{Outereach+1}.csv'\n",
    "    #             print(filename)\n",
    "    #             batch_df=DFOperationsStatement\n",
    "            # Export the current batch to a CSV file\n",
    "    #             batch_df.to_csv(filename, index=False)\n",
    "    #             batch_df.to_csv(DFOperationsStatement, index=False)\n",
    "else:\n",
    "    print(\"*\"*100)\n",
    "    print('None of the listed tables')\n",
    "    print(\"*\"*100)\n",
    "\n",
    "\n",
    "\n",
    "    CompanyFinancials = [OperationsStatement]#,TotalUStax,RandD]\n",
    "AllTablesOfACompany.append(CompanyFinancials)\n",
    "    \n",
    "\n",
    "# OuterDFOperationsStatement= pd.concat([OuterDFOperationsStatement, DFOperationsStatement])\n",
    "\n",
    "\n",
    "end_AllTables = time.time()\n",
    "\n",
    "timeTaken = end_AllTables-start_AllTables\n",
    "\n",
    "DFOperationsStatement1.to_csv('C:/UCLAAnderson/10KfilingsTables/Data/AutomatedFinancialData/July17_SP223to503WikiCompanies_Part4b_2023_withMetricsnDate2021nUnits_RevenueRnD_SP500.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statements_data[7898]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DFOperationsStatement1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# round(len(statements_data)/2)\n",
    "# len(df)\n",
    "len(set(DFOperationsStatement1['Company']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Start with empty dataframes:\n",
    "\n",
    "DFOperationsStatement2=pd.DataFrame()\n",
    "OuterDFOperationsStatement=pd.DataFrame()\n",
    "DF_RandD=pd.DataFrame()\n",
    "DFTaxStats=pd.DataFrame()\n",
    "\n",
    "##and with empty lists:\n",
    "List_RandD=[]\n",
    "ListTaxStats=[]\n",
    "Company10KData=[]\n",
    "CompanyFinancials=[]\n",
    "OperationsStatement=[]\n",
    "TotalUStax=[]\n",
    "ResearchDev=[]\n",
    "RandD=[]\n",
    "TaxnOperationsStatement=[]\n",
    "TaxOperationsRnD_Statement=[]\n",
    "Ops_sel=[]\n",
    "NetRevenue=[]\n",
    "NetSales=[]\n",
    "OperatingCost=[]\n",
    "EmployeeWages=[]\n",
    "df1B=pd.DataFrame()\n",
    "df_out1B=pd.DataFrame()\n",
    "df_Revenue=pd.DataFrame()\n",
    "\n",
    "##Iterating over a list of dictionaries\n",
    "# for each in list(range(0,len(statements_data),1)):#     for key in statements_data[index]:\n",
    "# worklist = list(range(0,len(statements_data),1))\n",
    "# batchsize = 500\n",
    "\n",
    "# for Outereach in range(0, len(worklist), batchsize):\n",
    "#     batch = worklist[Outereach:Outereach+batchsize] # the result might be shorter than batchsize at the end\n",
    "    # do stuff with batch\n",
    "    ## Grabbing the proper components\n",
    "    ## headers and data are lists of lists so making it a list instead\n",
    "for each in list(range(((round(len(statements_data)/2))+1),len(statements_data))):#batch:\n",
    "    income_headers = statements_data[each]['headers']##for taking the dates stored as the second list in headers of \n",
    "                                                     ##Consolidated Statements of Operations and Comprehensive Income (Loss) - USD ($) $ in Thousands\n",
    "\n",
    "        ##The balance sheet has only one header but if you go to the income statement it has two headers \n",
    "        ##we take [1] because we were interested in the dates header which was the second element\n",
    "\n",
    "    income_data = statements_data[each]['data']\n",
    "    # income_data ##here we have each row and the corresponding value for each particular financial statement- different sections\n",
    "\n",
    "\n",
    "    ## Put the data in a dataframe\n",
    "    income_df = pd.DataFrame(income_data)\n",
    "\n",
    "    ##For displaying\n",
    "\n",
    "#     print('-'*100)\n",
    "#     print('Before Reindexing')\n",
    "#     print('-'*100)\n",
    "#     display(income_df.head(7)) ## here we see the problem\n",
    "    ## first problem is that index is actually the Net Revenue, cost of services etc....\n",
    "\n",
    "    ## So, define the index column, rename it, and we need to make sure to drop the old column once we reindex.\n",
    "\n",
    "    income_df.index = income_df[0]##column 0 is first column-- something like row.names in R\n",
    "\n",
    "\n",
    "\n",
    "    ## Sometimes before removing the first column to row.names instead of replacing you can also assign it a new intuitive name\n",
    "\n",
    "    income_df.index.name = 'Category'\n",
    "    ##now dropping the first column which is now a rowname or index:\n",
    "    income_df = income_df.drop(0, axis = 1) ##drop method takes the column number to be dropped and along which axis \n",
    "                                            ##- 1 means along column axis\n",
    "\n",
    "\n",
    "    ## Now, we see problem 2, which is the dollar signs -  which tells me that right now it is a string\n",
    "    ## this being a string is a problem because I can't do any calculations on a string\n",
    "    ## Additionally, the blank values should be represented as a non-value (Problem 3)\n",
    "    ## 4th problem: negative values in losses should be true negatives and not have $\n",
    "\n",
    "    ## So, running some replace operations on this dataframe\n",
    "\n",
    "    ## Get rid of the '$', '(', ')', and convert the '' to NaNs. @44:34\n",
    "    #dealing with replacing positive values\n",
    "    #then, dealing with negative values such that if you see closing bracket replace with minus sign\n",
    "    # finally, if you see empty cells, replace with NaNs non values\n",
    "    income_df = income_df.replace('[\\$,)]','', regex = True)\\\n",
    "    .replace('[(]','-', regex = True)\\\n",
    "    .replace('',np.nan, regex = True)\n",
    "\n",
    "\n",
    "    CompNameCIK=str()\n",
    "    NetRevenue=[]\n",
    "    NetSales=[]\n",
    "    OperatingCost=[]\n",
    "    EmployeeWages=[]\n",
    "#         OperatingCost=pd.DataFrame()\n",
    "#         Ops_sel=pd.DataFrame()\n",
    "#         ops=pd.DataFrame()\n",
    "    ##Now that we aren't relying on the cover page to get the name of \n",
    "\n",
    "    soupy=BeautifulSoup(requests.get(statements_urlDF[each]['LinkToAllTabs'][0], headers = headers).content,'html.parser')\n",
    "\n",
    "    node = soupy.find('span', class_='companyName')\n",
    "    if node is not None:\n",
    "        ementa = node.text\n",
    "        CompNameCIK=soupy.find('span', class_='companyName').text.strip().split('CIK:')\n",
    "        CompanyName = CompNameCIK[0]\n",
    "        CIK = CompNameCIK[1]\n",
    "#     else:\n",
    "#         ementa = None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if 'operation' in statements_data[each]['headers'][0][0].lower() or 'income' in statements_data[each]['headers'][0][0].lower() or 'earning' in statements_data[each]['headers'][0][0].lower():\n",
    "#         or 'cash flow' in statements_data[each]['headers'][0][0].lower():\n",
    "#     or 'consolidated' in statements_data[each]['headers'][0][0].lower() or 'statement' in statements_data[each]['headers'][0][0].lower() or 'sheet' in statements_data[each]['headers'][0][0].lower():\n",
    "\n",
    "\n",
    "\n",
    "####################################################################################################################   \n",
    "\n",
    "\n",
    "\n",
    "        NetRevenue = pd.DataFrame(statements_data[each]['data']).loc[pd.DataFrame(statements_data[each]['data']).iloc[:,0].apply(lambda x: x.lower()).str.contains('|'.join(['revenue','revenues',\n",
    "        'sale','sales','net loss','net (loss)',\n",
    "        'net interest income','total operating','interest income','net interest',\n",
    "#         'net cash','earning','net other',\n",
    "#         'asset','liabilit',\n",
    "#         'general and administrative','general & administrative',#selling, \n",
    "        'research and development','research & development', 'R&D','R & D',\n",
    "#         'effective tax rate','effective rate'\n",
    "])),:]\n",
    "\n",
    "#######Using \"in\" instead of str.contains:\n",
    "#         stat_df=pd.DataFrame(statements_data[each]['data'])\n",
    "#         NetRevenue = stat_df[stat_df[0].apply(lambda x: any(term in x.lower() for term in ['revenues', 'revenue','sale','sales',\n",
    "#                                                                                  'net loss','net (loss)','net interest income',\n",
    "#                                                                                  'total operating','interest income','net interest',\n",
    "#                                                                                            'research and development','research & development', 'R&D','R & D']))]\n",
    "\n",
    "#         df1=NetRevenue.replace(\"\\$\",'',regex=True)\n",
    "        df = NetRevenue[NetRevenue.columns].replace({'\\(':'-','\\)':'','':'','%':'','\\$':'',' ':'',',':'','[1]':'','[2]':''}, regex = True)\n",
    "    # using apply method\n",
    "        cols = df.columns.drop(df.columns[0])\n",
    "        df[cols]=df[cols].apply(lambda x: pd.to_numeric(x, errors='coerce'))####Converting non metric columns to numeric to be able to multiply later with\n",
    "\n",
    "\n",
    "        if len(df)!=0:\n",
    "            df_Revenue=df\n",
    "\n",
    "            if '$ in millions' in statements_data[each]['headers'][0][0].lower():# and '$' in NetRevenue:\n",
    "               #and NetRevenue>1000  \n",
    "            ##Cleaning the data before building it:\n",
    "                s =df_Revenue[df_Revenue.select_dtypes(include=['number']).columns]*1000000\n",
    "                df[s.columns] = s\n",
    "            elif '$ in thousands' in statements_data[each]['headers'][0][0].lower():# and '$' in NetRevenue:\n",
    "                s =df_Revenue[df_Revenue.select_dtypes(include=['number']).columns]*1000\n",
    "                df[s.columns] = s\n",
    "            elif '$ in billions' in statements_data[each]['headers'][0][0].lower():# and '$' in NetRevenue:\n",
    "                s =df_Revenue[df_Revenue.select_dtypes(include=['number']).columns]*1000000000\n",
    "                df[s.columns] = s\n",
    "                ## below is an exception\n",
    "    #         elif '$ in millions, $ in billions' in statements_data[each]['headers'][0][0].lower():# and '$' in NetRevenue:\n",
    "    #             # and NetRevenue<1000 \n",
    "    #             df_Revenue =df_Revenue[df_Revenue.select_dtypes(include=['number']).columns]*1000000000\n",
    "            else:\n",
    "                s=df_Revenue[df_Revenue.select_dtypes(include=['number']).columns]*1\n",
    "                df[s.columns] = s\n",
    "\n",
    "            df1B_duped = pd.concat([df[df.columns[0]], df_Revenue], axis=1)\n",
    "\n",
    "\n",
    "            ####Dedupe column names right here to change colnames ############\n",
    "\n",
    "\n",
    "            df1B_dedupe = df1B_duped.loc[:,~df1B_duped.columns.duplicated()].copy()\n",
    "\n",
    "            ##for later\n",
    "            df1B=df1B_dedupe.dropna(axis=1)\n",
    "    ## Below is to avoid reindex error due to repeated columns like deferred tax revenue being reported in two columns acc to: https://stackoverflow.com/questions/14984119/python-pandas-remove-duplicate-columns\n",
    "    #         df1B = pd.concat([df[df.columns[0]], df_Revenue], axis=1).loc[:,~(pd.concat([df[df.columns[0]], df_Revenue], axis=1)).columns.duplicated()].copy()\n",
    "\n",
    "    #             df1B = df_Rev[df_Rev.columns].replace({'\\(':'-','\\)':'','':'','%':'','\\$':'',' ':'',',':''}, regex = True)\n",
    "        #         '\\$':''<<-removed this condition because if there is $ in front of the value, the headers direct the units thousand or million\n",
    "\n",
    "                #         if '$'in df1B:\n",
    "                    #multiply bymillion or thousand\n",
    "            df_out1B=pd.DataFrame()\n",
    "            ##2. Replace NanNs with next column's non-NaN value\n",
    "            if df1B.isnull().values.any():\n",
    "                df_out1B = df1B.fillna(method='ffill',axis=0).dropna(axis=1)\n",
    "    #                 df_out1B = df1B.apply(lambda x: pd.Series(x.dropna().to_numpy()), axis=1)\n",
    "    #                 df_out1B = df_out1B.set_axis(df1B.columns[:df_out1B.shape[1]], axis=1).reindex(df1B.columns, axis=1).dropna(axis=1)\n",
    "            else:\n",
    "                df_out1B = df1B.dropna(axis=1)\n",
    "            print(df_out1B)\n",
    "\n",
    "            ##3. replacing empty whitespace with NaN---do we need to do this again?\n",
    "            cleaned_NetRevenue=df_out1B.replace(r'^\\s*$', np.nan, regex=True).dropna(axis=1, how='all')\n",
    "\n",
    "\n",
    "            if len(CompNameCIK)!=0:\n",
    "                cleaned_NetRevenue['Company']= CompanyName\n",
    "                cleaned_NetRevenue['cik_str']= CIK\n",
    "            else:\n",
    "                cleaned_NetRevenue['Company']= 'NotFound'\n",
    "                cleaned_NetRevenue['cik_str']= 'NotFound'\n",
    "            print(cleaned_NetRevenue) \n",
    "\n",
    "    #######################To avoid the initial columns being 0,1,2,3 followed with opsheader dates##############\n",
    "\n",
    "\n",
    "    ##################################################################################\n",
    "        ##To avoid clumping all dates into a single list:\n",
    "            if len(statements_data[each]['headers'])>1:\n",
    "                        dates_init = statements_data[each]['headers'][1]\n",
    "            else:\n",
    "                dates_init = statements_data[each]['headers'][0]\n",
    "\n",
    "\n",
    "            dateslist_initial=[]\n",
    "            for onedate in dates_init:\n",
    "            #     print(re.sub(r'(,?!(([^\"]*\"){2})*[^\"]*$)',':', str(onedate)))\n",
    "                mystring1 = re.sub(r'(,?!(([^\"]*\"){2})*[^\"]*$)',':', str(onedate))\n",
    "                print(re.sub(\",\", \"/\", mystring1))\n",
    "                datestr1a=re.sub(\",\", \"/\", mystring1)## replace comma with / before the year\n",
    "                if len(datestr1a.split(\"/\"))>1:\n",
    "                    datestr1=datestr1a.split(\"/\")[0].split(\" \")[0]+\" \"+datestr1a.split(\"/\")[1]\n",
    "                    ##To drop the exact date, splitting at \\ for year and then splitting at space to keep Month\n",
    "                else:\n",
    "                    datestr1=datestr1a.split(\"/\")[0].split(\" \")[0]+\" \"+datestr1a.split(\"/\")[0]\n",
    "                dateslist_initial.append(datestr1)\n",
    "\n",
    "            print(dateslist_initial)\n",
    "            singledatesstring1 = ','.join(dateslist_initial)\n",
    "            datesList1_init = re.sub(\",\", \",\", singledatesstring1).split(',')\n",
    "\n",
    "            CleanRevHeaders = ['Metric']\n",
    "\n",
    "\n",
    "            for each1 in datesList1_init:\n",
    "                print('-'.join(each1.split(',')))\n",
    "                CleanRevHeaders.append(''.join(each1.split(',')))\n",
    "\n",
    "\n",
    "            import itertools\n",
    "            AllCleanRevHeaders = list(itertools.chain(CleanRevHeaders, ['Company'], ['cik_str']))\n",
    "\n",
    "            if len(cleaned_NetRevenue.columns.tolist())==len(AllCleanRevHeaders):\n",
    "                cleaned_NetRevenue.columns=AllCleanRevHeaders\n",
    "\n",
    "            print(cleaned_NetRevenue)\n",
    "        ##################################################################################\n",
    "\n",
    "    ####################################################################################################################\n",
    "\n",
    "\n",
    "    ##Some tables might not be there like R&D for some companies:\n",
    "            if (len(cleaned_NetRevenue))!=0:#+len(cleaned_NetSales)+len(cleaned_OperatingCost)+len(cleaned_EmployeeWages)+len(cleaned_RandDOpsAmgen))!=0:\n",
    "                #ops = concat(*(cleaned_NetRevenue))#, cleaned_NetSales,cleaned_OperatingCost, cleaned_EmployeeWages,cleaned_RandDOpsAmgen))\n",
    "                ops=cleaned_NetRevenue\n",
    "                if len(ops)!=0:\n",
    "\n",
    "                    editedOps = pd.DataFrame(ops[ops.columns[ops.notnull().all()]].iloc[0].dropna())\n",
    "                    editedOps.dropna(inplace=True)\n",
    "                    editedOps\n",
    "                    # remove empty column that just takes up space and shifts the 3 years' data one cell further\n",
    "                    CleanOps = editedOps.loc[:, editedOps.notna().any()]\n",
    "                    Operations_edit = pd.DataFrame(CleanOps.replace(r'^\\s*$%', np.nan, regex=True).dropna())\n",
    "                    #TotalUStax_edit.where(~TotalUStax_edit.apply(lambda x: x.astype(str).str.contains('|'.join(['%','$'])))).dropna().T\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #             Ops_sel = [ops]\n",
    "                    Ops_sel = Operations_edit.T\n",
    "        #             dates = statements_data[each]['headers'][0]\n",
    "                    if len(statements_data[each]['headers'])>1:\n",
    "                        dates = statements_data[each]['headers'][1]\n",
    "                    else:\n",
    "                        dates = statements_data[each]['headers'][0]\n",
    "\n",
    "    #                     Ops_sel.columns = ['Metric',dates,'Company','cik_str']##<<<<to see if date can be registered\n",
    "\n",
    "\n",
    "        ##################################################################################\n",
    "        ##To avoid clumping all dates into a single list:\n",
    "\n",
    "                    dateslist1=[]\n",
    "                    for onedate in dates:\n",
    "                    #     print(re.sub(r'(,?!(([^\"]*\"){2})*[^\"]*$)',':', str(onedate)))\n",
    "                        mystring1 = re.sub(r'(,?!(([^\"]*\"){2})*[^\"]*$)',':', str(onedate))\n",
    "                        print(re.sub(\",\", \"/\", mystring1))\n",
    "                        datestr1a=re.sub(\",\", \"/\", mystring1)## replace comma with / before the year\n",
    "                        if len(datestr1a.split(\"/\"))>1:\n",
    "                            datestr1=datestr1a.split(\"/\")[0].split(\" \")[0]+\" \"+datestr1a.split(\"/\")[1]\n",
    "                            ##To drop the exact date, splitting at \\ for year and then splitting at space to keep Month\n",
    "                        else:\n",
    "                            datestr1=datestr1a.split(\"/\")[0].split(\" \")[0]+\" \"+datestr1a.split(\"/\")[0]\n",
    "                        dateslist1.append(datestr1)\n",
    "\n",
    "                    print(dateslist1)\n",
    "                    singledatesstring1 = ','.join(dateslist1)\n",
    "                    datesList1 = re.sub(\",\", \",\", singledatesstring1).split(',')\n",
    "\n",
    "                    OpsHeaders = ['Metric']\n",
    "\n",
    "\n",
    "                    for each1 in datesList1:\n",
    "                        print('-'.join(each1.split(',')))\n",
    "                        OpsHeaders.append(''.join(each1.split(',')))\n",
    "\n",
    "\n",
    "                    import itertools\n",
    "                    AllOpsHeaders = list(itertools.chain(OpsHeaders, ['Company'], ['cik_str']))\n",
    "\n",
    "                    if len(Ops_sel.columns.tolist())==len(AllOpsHeaders):\n",
    "                        Ops_sel.columns=AllOpsHeaders\n",
    "        ##################################################################################\n",
    "        #         else:\n",
    "        #             Ops_sel = pd.DataFrame(['NIL'])\n",
    "\n",
    "        ##### Checking for duplicated columns like deferred revenues that create problems in rbind:\n",
    "\n",
    "            Ops_sel_dedupe = Ops_sel.loc[:,~Ops_sel.columns.duplicated()].copy()\n",
    "            OperationsStatement.append(Ops_sel_dedupe)\n",
    "            print(OperationsStatement)\n",
    "\n",
    "\n",
    "            ##Also creating a dataframe instead of a list of dataframes for easier understanding of column values\n",
    "            DFOperationsStatement2 = pd.concat([DFOperationsStatement2, Ops_sel_dedupe])\n",
    "\n",
    "            # Define the filename for the current batch\n",
    "    #             filename = f'batch_{each+1}_{Outereach+1}.csv'\n",
    "    #             print(filename)\n",
    "    #             batch_df=DFOperationsStatement\n",
    "            # Export the current batch to a CSV file\n",
    "    #             batch_df.to_csv(filename, index=False)\n",
    "    #             batch_df.to_csv(DFOperationsStatement, index=False)\n",
    "else:\n",
    "    print(\"*\"*100)\n",
    "    print('None of the listed tables')\n",
    "    print(\"*\"*100)\n",
    "\n",
    "\n",
    "\n",
    "    CompanyFinancials = [OperationsStatement]#,TotalUStax,RandD]\n",
    "AllTablesOfACompany.append(CompanyFinancials)\n",
    "    \n",
    "\n",
    "# OuterDFOperationsStatement= pd.concat([OuterDFOperationsStatement, DFOperationsStatement])\n",
    "\n",
    "\n",
    "end_AllTables = time.time()\n",
    "\n",
    "timeTaken = end_AllTables-start_AllTables\n",
    "\n",
    "DFOperationsStatement2.to_csv('C:/UCLAAnderson/10KfilingsTables/Data/AutomatedFinancialData/May19B_Part2_2023_withMetricsnDate2021nUnits_RevenueRnD_SP500.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DFOperationsStatement.to_csv('C:/UCLAAnderson/10KfilingsTables/Data/AutomatedFinancialData/May17B_2023_withMetricsnDate2021nUnits_RevenueRnD_SP500.csv', index=False)\n",
    "print(timeTaken/60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(statements_data)\n",
    "list(range(((round(len(statements_data)/2))+1),len(statements_data)))\n",
    "list(range(0,round(len(statements_data)/2),1))\n",
    "round(len(statements_data)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(timeTaken/60)\n",
    "cleaned_NetRevenue\n",
    "df_Revenue\n",
    "NetRevenue\n",
    "statements_data[each]\n",
    "DFOperationsStatement.tail(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statements_data[each]\n",
    "# NetRevenue\n",
    "# pd.DataFrame(statements_data[each]['data']).loc[pd.DataFrame(statements_data[each]['data']).iloc[:,0].apply(lambda x: x.lower()).str.contains('|'.join(['revenue','revenues',\n",
    "#     'sale','sales','net loss','net (loss)',\n",
    "#     'net interest income','total operating','interest income','net interest','research and development','research & development', 'R&D','R & D'\n",
    "# #         'net cash','earning','net other',\n",
    "# #         'asset','liabilit',\n",
    "# #         'general and administrative','general & administrative',#selling, \n",
    "    \n",
    "# #         'effective tax rate','effective rate'\n",
    "# ])),:]\n",
    "df1B_dedupe\n",
    "cleaned_NetRevenue\n",
    "Ops_sel\n",
    "\n",
    "len(NetRevenue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting the output #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DFOperationsStatement=pd.concat(DFOperationsStatement1,DFOperationsStatement2)\n",
    "DFOperationsStatement.to_csv('C:/UCLAAnderson/10KfilingsTables/Data/AutomatedFinancialData/May19_2023_withMetricsnDate2021nUnits_RevenueRnD_SP563.csv', index=False)\n",
    "\n",
    "# OuterDFOperationsStatement.to_csv('C:/UCLAAnderson/10KfilingsTables/Data/AutomatedFinancialData/May11B_2023_AllwithMetricsnDate2021nUnits_RevenueRnD_SP563.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AllTablesOfACompany"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import concurrent.futures\n",
    "# from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_statement_data(statement_data):\n",
    "# #     for each in list(range(0,len(statements_data),1)):#batch:\n",
    "#     income_headers = statements_data['headers']##for taking the dates stored as the second list in headers of \n",
    "#                                                      ##Consolidated Statements of Operations and Comprehensive Income (Loss) - USD ($) $ in Thousands\n",
    "\n",
    "#         ##The balance sheet has only one header but if you go to the income statement it has two headers \n",
    "#         ##we take [1] because we were interested in the dates header which was the second element\n",
    "\n",
    "#     income_data = statements_data['data']\n",
    "#     # income_data ##here we have each row and the corresponding value for each particular financial statement- different sections\n",
    "\n",
    "\n",
    "#     ## Put the data in a dataframe\n",
    "#     income_df = pd.DataFrame(income_data)\n",
    "\n",
    "#     ## So, define the index column, rename it, and we need to make sure to drop the old column once we reindex.\n",
    "\n",
    "#     income_df.index = income_df[0]##column 0 is first column-- something like row.names in R\n",
    "\n",
    "#     ## Sometimes before removing the first column to row.names instead of replacing you can also assign it a new intuitive name\n",
    "\n",
    "#     income_df.index.name = 'Category'\n",
    "#     ##now dropping the first column which is now a rowname or index:\n",
    "#     income_df = income_df.drop(0, axis = 1) ##drop method takes the column number to be dropped and along which axis \n",
    "#                                             ##- 1 means along column axis\n",
    "\n",
    "#     ## Now, we see problem 2, which is the dollar signs -  which tells me that right now it is a string\n",
    "#     ## this being a string is a problem because I can't do any calculations on a string\n",
    "#     ## Additionally, the blank values should be represented as a non-value (Problem 3)\n",
    "#     ## 4th problem: negative values in losses should be true negatives and not have $\n",
    "\n",
    "#     ## So, running some replace operations on this dataframe\n",
    "\n",
    "#     ## Get rid of the '$', '(', ')', and convert the '' to NaNs. @44:34\n",
    "#     #dealing with replacing positive values\n",
    "#     #then, dealing with negative values such that if you see closing bracket replace with minus sign\n",
    "#     # finally, if you see empty cells, replace with NaNs non values\n",
    "#     income_df = income_df.replace('[\\$,)]','', regex = True)\\\n",
    "#     .replace('[(]','-', regex = True)\\\n",
    "#     .replace('',np.nan, regex = True)\n",
    "\n",
    "\n",
    "#     CompNameCIK=str()\n",
    "#     NetRevenue=[]\n",
    "#     NetSales=[]\n",
    "#     OperatingCost=[]\n",
    "#     EmployeeWages=[]\n",
    "# #         OperatingCost=pd.DataFrame()\n",
    "# #         Ops_sel=pd.DataFrame()\n",
    "# #         ops=pd.DataFrame()\n",
    "#     ##Now that we aren't relying on the cover page to get the name of \n",
    "\n",
    "# #     soupy=BeautifulSoup(requests.get(statements_urlDF['LinkToAllTabs'][0], headers = headers).content,'html.parser')\n",
    "#     # Iterate over the list of URLs\n",
    "#     urlDF=pd.concat(statements_urlDF)\n",
    "#     for url in urlDF:\n",
    "#         soupy = BeautifulSoup(requests.get(url, headers=headers).content, 'html.parser')\n",
    "#         node = soupy.find('span', class_='companyName')\n",
    "#         if node is not None:\n",
    "#             ementa = node.text\n",
    "#             CompNameCIK=soupy.find('span', class_='companyName').text.strip().split('CIK:')\n",
    "#             CompanyName = CompNameCIK[0]\n",
    "#             CIK = CompNameCIK[1]\n",
    "# #     else:\n",
    "# #         ementa = None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     if 'operation' in statements_data['headers'][0][0].lower() or 'income' in statements_data['headers'][0][0].lower():\n",
    "# #         or 'cash flow' in statements_data[each]['headers'][0][0].lower():\n",
    "# #     or 'consolidated' in statements_data[each]['headers'][0][0].lower() or 'statement' in statements_data[each]['headers'][0][0].lower() or 'sheet' in statements_data[each]['headers'][0][0].lower():\n",
    "\n",
    "\n",
    "\n",
    "# ####################################################################################################################   \n",
    "\n",
    "\n",
    "\n",
    "#         NetRevenue = pd.DataFrame(statements_data['data']).loc[pd.DataFrame(statements_data['data']).iloc[:,0].apply(lambda x: x.lower()).str.contains('|'.join(['revenue',\n",
    "#         'net income','sale','net loss','net (loss)','total sales',\n",
    "#         'net interest income','total operating','interest income','net interest',\n",
    "# #         'net cash','earning','net other',\n",
    "# #         'asset','liabilit',\n",
    "# #         'general and administrative','general & administrative',#selling, \n",
    "#         'research and development','research & development', 'R&D','R & D',\n",
    "# #         'effective tax rate','effective rate'\n",
    "# ])),:]\n",
    "\n",
    "\n",
    "# #         df1=NetRevenue.replace(\"\\$\",'',regex=True)\n",
    "#         df = NetRevenue[NetRevenue.columns].replace({'\\(':'-','\\)':'','':'','%':'','\\$':'',' ':'',',':'','[1]':'','[2]':''}, regex = True)\n",
    "#     # using apply method\n",
    "#         cols = df.columns.drop(df.columns[0])\n",
    "#         df[cols]=df[cols].apply(lambda x: pd.to_numeric(x, errors='coerce'))####Converting non metric columns to numeric to be able to multiply later with\n",
    "#         df_Revenue=df\n",
    "\n",
    "#         if '$ in millions' in statements_data['headers'][0][0].lower():# and '$' in NetRevenue:\n",
    "#            #and NetRevenue>1000  \n",
    "#         ##Cleaning the data before building it:\n",
    "#             s =df_Revenue[df_Revenue.select_dtypes(include=['number']).columns]*1000000\n",
    "#             df[s.columns] = s\n",
    "#         elif '$ in thousands' in statements_data['headers'][0][0].lower():# and '$' in NetRevenue:\n",
    "#             s =df_Revenue[df_Revenue.select_dtypes(include=['number']).columns]*1000\n",
    "#             df[s.columns] = s\n",
    "#         elif '$ in billions' in statements_data['headers'][0][0].lower():# and '$' in NetRevenue:\n",
    "#             s =df_Revenue[df_Revenue.select_dtypes(include=['number']).columns]*1000000000\n",
    "#             df[s.columns] = s\n",
    "#             ## below is an exception\n",
    "# #         elif '$ in millions, $ in billions' in statements_data[each]['headers'][0][0].lower():# and '$' in NetRevenue:\n",
    "# #             # and NetRevenue<1000 \n",
    "# #             df_Revenue =df_Revenue[df_Revenue.select_dtypes(include=['number']).columns]*1000000000\n",
    "#         else:\n",
    "#             s=df_Revenue[df_Revenue.select_dtypes(include=['number']).columns]*1\n",
    "#             df[s.columns] = s\n",
    "# #             df[df.select_dtypes(include=['number']).columns] *= 3\n",
    "\n",
    "# #         print(df_Revenue)\n",
    "\n",
    "#             ## After this, we can remove the $ sign\n",
    "#             ##1. Remove any/all % and $ signs that take up separate columns or within a column preceding the value because we are only studying US$\n",
    "# #             df1B=df_Revenue[df_Revenue.columns].replace({'\\(':'-','\\)':'','':'','%':'','\\$':''}, regex = True)## need to precede the dollar sign with a \\ to be identified in regex\n",
    "\n",
    "#             ###Concatenate cbind the net sales/ revenue/ R&D terminologies with multiplied values:\n",
    "# #         df1B = df_Revenue\n",
    "# #         df = df.reset_index(drop=True)\n",
    "#         df1B_duped = pd.concat([df[df.columns[0]], df_Revenue], axis=1)\n",
    "\n",
    "\n",
    "#         ####Dedupe column names right here to change colnames ############\n",
    "\n",
    "\n",
    "#         df1B_dedupe = df1B_duped.loc[:,~df1B_duped.columns.duplicated()].copy()\n",
    "\n",
    "#         ##for later\n",
    "#         df1B=df1B_dedupe.dropna()\n",
    "# ## Below is to avoid reindex error due to repeated columns like deferred tax revenue being reported in two columns acc to: https://stackoverflow.com/questions/14984119/python-pandas-remove-duplicate-columns\n",
    "# #         df1B = pd.concat([df[df.columns[0]], df_Revenue], axis=1).loc[:,~(pd.concat([df[df.columns[0]], df_Revenue], axis=1)).columns.duplicated()].copy()\n",
    "\n",
    "# #             df1B = df_Rev[df_Rev.columns].replace({'\\(':'-','\\)':'','':'','%':'','\\$':'',' ':'',',':''}, regex = True)\n",
    "#     #         '\\$':''<<-removed this condition because if there is $ in front of the value, the headers direct the units thousand or million\n",
    "\n",
    "#             #         if '$'in df1B:\n",
    "#                 #multiply bymillion or thousand\n",
    "#         df_out1B=pd.DataFrame()\n",
    "#         ##2. Replace NanNs with next column's non-NaN value\n",
    "#         if df1B.isnull().values.any():\n",
    "#             df_out1B = df1B.fillna(method='ffill',axis=0).dropna(axis=1)\n",
    "# #                 df_out1B = df1B.apply(lambda x: pd.Series(x.dropna().to_numpy()), axis=1)\n",
    "# #                 df_out1B = df_out1B.set_axis(df1B.columns[:df_out1B.shape[1]], axis=1).reindex(df1B.columns, axis=1).dropna(axis=1)\n",
    "#         else:\n",
    "#             df_out1B = df1B.dropna(axis=1)\n",
    "#         print(df_out1B)\n",
    "\n",
    "#         ##3. replacing empty whitespace with NaN---do we need to do this again?\n",
    "#         cleaned_NetRevenue=df_out1B.replace(r'^\\s*$', np.nan, regex=True).dropna(axis=1, how='all')\n",
    "\n",
    "\n",
    "#         if len(CompNameCIK)!=0:\n",
    "#             cleaned_NetRevenue['Company']= CompanyName\n",
    "#             cleaned_NetRevenue['cik_str']= CIK\n",
    "#         else:\n",
    "#             cleaned_NetRevenue['Company']= 'NotFound'\n",
    "#             cleaned_NetRevenue['cik_str']= 'NotFound'\n",
    "#         print(cleaned_NetRevenue) \n",
    "\n",
    "# #######################To avoid the initial columns being 0,1,2,3 followed with opsheader dates##############\n",
    "\n",
    "\n",
    "# ##################################################################################\n",
    "#     ##To avoid clumping all dates into a single list:\n",
    "#         if len(statements_data['headers'])>1:\n",
    "#                     dates_init = statements_data['headers'][1]\n",
    "#         else:\n",
    "#             dates_init = statements_data['headers'][0]\n",
    "\n",
    "\n",
    "#         dateslist_initial=[]\n",
    "#         for onedate in dates_init:\n",
    "#         #     print(re.sub(r'(,?!(([^\"]*\"){2})*[^\"]*$)',':', str(onedate)))\n",
    "#             mystring1 = re.sub(r'(,?!(([^\"]*\"){2})*[^\"]*$)',':', str(onedate))\n",
    "#             print(re.sub(\",\", \"/\", mystring1))\n",
    "#             datestr1a=re.sub(\",\", \"/\", mystring1)## replace comma with / before the year\n",
    "#             if len(datestr1a.split(\"/\"))>1:\n",
    "#                 datestr1=datestr1a.split(\"/\")[0].split(\" \")[0]+\" \"+datestr1a.split(\"/\")[1]\n",
    "#                 ##To drop the exact date, splitting at \\ for year and then splitting at space to keep Month\n",
    "#             else:\n",
    "#                 datestr1=datestr1a.split(\"/\")[0].split(\" \")[0]+\" \"+datestr1a.split(\"/\")[0]\n",
    "#             dateslist_initial.append(datestr1)\n",
    "\n",
    "#         print(dateslist_initial)\n",
    "#         singledatesstring1 = ','.join(dateslist_initial)\n",
    "#         datesList1_init = re.sub(\",\", \",\", singledatesstring1).split(',')\n",
    "\n",
    "#         CleanRevHeaders = ['Metric']\n",
    "\n",
    "\n",
    "#         for each1 in datesList1_init:\n",
    "#             print('-'.join(each1.split(',')))\n",
    "#             CleanRevHeaders.append(''.join(each1.split(',')))\n",
    "\n",
    "\n",
    "#         import itertools\n",
    "#         AllCleanRevHeaders = list(itertools.chain(CleanRevHeaders, ['Company'], ['cik_str']))\n",
    "\n",
    "#         if len(cleaned_NetRevenue.columns.tolist())==len(AllCleanRevHeaders):\n",
    "#             cleaned_NetRevenue.columns=AllCleanRevHeaders\n",
    "\n",
    "        \n",
    "#     ##################################################################################\n",
    "\n",
    "# ####################################################################################################################\n",
    "\n",
    "\n",
    "# ##Some tables might not be there like R&D for some companies:\n",
    "#         if (len(cleaned_NetRevenue))!=0:#+len(cleaned_NetSales)+len(cleaned_OperatingCost)+len(cleaned_EmployeeWages)+len(cleaned_RandDOpsAmgen))!=0:\n",
    "#             #ops = concat(*(cleaned_NetRevenue))#, cleaned_NetSales,cleaned_OperatingCost, cleaned_EmployeeWages,cleaned_RandDOpsAmgen))\n",
    "#             ops=cleaned_NetRevenue\n",
    "#             if len(ops)!=0:\n",
    "\n",
    "#                 editedOps = pd.DataFrame(ops[ops.columns[ops.notnull().all()]].iloc[0].dropna())\n",
    "#                 editedOps.dropna(inplace=True)\n",
    "#                 editedOps\n",
    "#                 # remove empty column that just takes up space and shifts the 3 years' data one cell further\n",
    "#                 CleanOps = editedOps.loc[:, editedOps.notna().any()]\n",
    "#                 Operations_edit = pd.DataFrame(CleanOps.replace(r'^\\s*$%', np.nan, regex=True).dropna())\n",
    "#                 #TotalUStax_edit.where(~TotalUStax_edit.apply(lambda x: x.astype(str).str.contains('|'.join(['%','$'])))).dropna().T\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     #             Ops_sel = [ops]\n",
    "#                 Ops_sel = Operations_edit.T\n",
    "#     #             dates = statements_data[each]['headers'][0]\n",
    "#                 if len(statements_data['headers'])>1:\n",
    "#                     dates = statements_data['headers'][1]\n",
    "#                 else:\n",
    "#                     dates = statements_data['headers'][0]\n",
    "\n",
    "# #                     Ops_sel.columns = ['Metric',dates,'Company','cik_str']##<<<<to see if date can be registered\n",
    "\n",
    "\n",
    "#     ##################################################################################\n",
    "#     ##To avoid clumping all dates into a single list:\n",
    "\n",
    "#                 dateslist1=[]\n",
    "#                 for onedate in dates:\n",
    "#                 #     print(re.sub(r'(,?!(([^\"]*\"){2})*[^\"]*$)',':', str(onedate)))\n",
    "#                     mystring1 = re.sub(r'(,?!(([^\"]*\"){2})*[^\"]*$)',':', str(onedate))\n",
    "#                     print(re.sub(\",\", \"/\", mystring1))\n",
    "#                     datestr1a=re.sub(\",\", \"/\", mystring1)## replace comma with / before the year\n",
    "#                     if len(datestr1a.split(\"/\"))>1:\n",
    "#                         datestr1=datestr1a.split(\"/\")[0].split(\" \")[0]+\" \"+datestr1a.split(\"/\")[1]\n",
    "#                         ##To drop the exact date, splitting at \\ for year and then splitting at space to keep Month\n",
    "#                     else:\n",
    "#                         datestr1=datestr1a.split(\"/\")[0].split(\" \")[0]+\" \"+datestr1a.split(\"/\")[0]\n",
    "#                     dateslist1.append(datestr1)\n",
    "\n",
    "#                 print(dateslist1)\n",
    "#                 singledatesstring1 = ','.join(dateslist1)\n",
    "#                 datesList1 = re.sub(\",\", \",\", singledatesstring1).split(',')\n",
    "\n",
    "#                 OpsHeaders = ['Metric']\n",
    "\n",
    "\n",
    "#                 for each1 in datesList1:\n",
    "#                     print('-'.join(each1.split(',')))\n",
    "#                     OpsHeaders.append(''.join(each1.split(',')))\n",
    "\n",
    "\n",
    "#                 import itertools\n",
    "#                 AllOpsHeaders = list(itertools.chain(OpsHeaders, ['Company'], ['cik_str']))\n",
    "\n",
    "#                 if len(Ops_sel.columns.tolist())==len(AllOpsHeaders):\n",
    "#                     Ops_sel.columns=AllOpsHeaders\n",
    "#     ##################################################################################\n",
    "#     #         else:\n",
    "#     #             Ops_sel = pd.DataFrame(['NIL'])\n",
    "\n",
    "#     ##### Checking for duplicated columns like deferred revenues that create problems in rbind:\n",
    "\n",
    "#         Ops_sel_dedupe = Ops_sel.loc[:,~Ops_sel.columns.duplicated()].copy()\n",
    "#         OperationsStatement.append(Ops_sel_dedupe)\n",
    "#         print(OperationsStatement)\n",
    "\n",
    "\n",
    "#         ##Also creating a dataframe instead of a list of dataframes for easier understanding of column values\n",
    "#         DFOperationsStatement = pd.concat([DFOperationsStatement, Ops_sel_dedupe])\n",
    "\n",
    "#         # Define the filename for the current batch\n",
    "# #             filename = f'batch_{each+1}_{Outereach+1}.csv'\n",
    "# #             print(filename)\n",
    "# #             batch_df=DFOperationsStatement\n",
    "#         # Export the current batch to a CSV file\n",
    "# #             batch_df.to_csv(filename, index=False)\n",
    "# #             batch_df.to_csv(DFOperationsStatement, index=False)\n",
    "#     return(DFOperationsStatement)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a list of statement data\n",
    "# statement_data_list = statements_data\n",
    "\n",
    "# # Define the number of concurrent workers (adjust as needed)\n",
    "# num_workers = 4\n",
    "\n",
    "# # Create a ThreadPoolExecutor with the desired number of workers\n",
    "# executor = concurrent.futures.ThreadPoolExecutor(max_workers=num_workers)\n",
    "\n",
    "# # Submit the tasks to the executor\n",
    "# # Use partial to create a new function with a single argument for statement_data\n",
    "# # Pass the statement_data_list to map for parallel processing\n",
    "# future_results = executor.map(partial(process_statement_data), statement_data_list)\n",
    "\n",
    "# # Retrieve the results from the futures\n",
    "# results = list(future_results)\n",
    "\n",
    "# # Shutdown the executor to free resources\n",
    "# executor.shutdown()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### --------------- end parallel processing -----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "************************************************************************"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
